#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºFAISSçš„å‘é‡æ£€ç´¢RAGç³»ç»Ÿ
"""

import re
import json
import logging
import pickle
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

logger = logging.getLogger(__name__)

class FAISSRetriever:
    """åŸºäºFAISSçš„å‘é‡æ£€ç´¢å™¨"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", 
                 index_path: str = "data/faiss_index", 
                 chunk_size: int = 512, 
                 overlap: int = 50):
        """
        åˆå§‹åŒ–FAISSæ£€ç´¢å™¨
        
        Args:
            model_name: å¥å­åµŒå…¥æ¨¡å‹åç§°
            index_path: FAISSç´¢å¼•ä¿å­˜è·¯å¾„
            chunk_size: æ–‡æœ¬å—å¤§å°
            overlap: æ–‡æœ¬å—é‡å å¤§å°
        """
        self.model_name = model_name
        self.index_path = Path(index_path)
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.index = None
        self.chunks = []
        self.chunk_metadata = []
        self.dimension = 384  # all-MiniLM-L6-v2çš„ç»´åº¦
        
        # åˆ›å»ºç´¢å¼•ç›®å½•
        self.index_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–
        self._initialize_model()
        self._load_or_build_index()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹"""
        try:
            self.model = SentenceTransformer(self.model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            logger.info(f"æˆåŠŸåˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹: {self.model_name}, ç»´åº¦: {self.dimension}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def _load_or_build_index(self):
        """åŠ è½½æˆ–æ„å»ºFAISSç´¢å¼•"""
        index_file = self.index_path / "faiss_index.bin"
        chunks_file = self.index_path / "chunks.json"
        metadata_file = self.index_path / "metadata.json"
        
        if index_file.exists() and chunks_file.exists() and metadata_file.exists():
            try:
                # åŠ è½½ç°æœ‰ç´¢å¼•
                self.index = faiss.read_index(str(index_file))
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    self.chunks = json.load(f)
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    self.chunk_metadata = json.load(f)
                logger.info(f"æˆåŠŸåŠ è½½ç°æœ‰FAISSç´¢å¼•ï¼ŒåŒ…å« {len(self.chunks)} ä¸ªæ–‡æ¡£å—")
            except Exception as e:
                logger.warning(f"åŠ è½½ç°æœ‰ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†é‡æ–°æ„å»º")
                self._build_index()
        else:
            logger.info("æœªæ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•")
            self._build_index()
    
    def _build_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        try:
            # åŠ è½½å’Œé¢„å¤„ç†æ–‡æ¡£
            documents = self._load_documents()
            logger.info(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
            
            # åˆ†å‰²æ–‡æ¡£ä¸ºå—
            self.chunks, self.chunk_metadata = self._split_documents(documents)
            logger.info(f"åˆ†å‰²ä¸º {len(self.chunks)} ä¸ªæ–‡æ¡£å—")
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("å¼€å§‹ç”Ÿæˆæ–‡æ¡£åµŒå…¥å‘é‡...")
            embeddings = []
            batch_size = 32
            
            for i in tqdm(range(0, len(self.chunks), batch_size), desc="ç”ŸæˆåµŒå…¥"):
                batch_chunks = self.chunks[i:i + batch_size]
                batch_embeddings = self.model.encode(batch_chunks, convert_to_numpy=True)
                embeddings.append(batch_embeddings)
            
            # åˆå¹¶æ‰€æœ‰åµŒå…¥
            all_embeddings = np.vstack(embeddings)
            logger.info(f"ç”Ÿæˆäº† {all_embeddings.shape[0]} ä¸ªåµŒå…¥å‘é‡")
            
            # åˆ›å»ºFAISSç´¢å¼•
            self.index = faiss.IndexFlatIP(self.dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
            self.index.add(all_embeddings.astype('float32'))
            
            # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
            faiss.write_index(self.index, str(self.index_path / "faiss_index.bin"))
            with open(self.index_path / "chunks.json", 'w', encoding='utf-8') as f:
                json.dump(self.chunks, f, ensure_ascii=False, indent=2)
            with open(self.index_path / "metadata.json", 'w', encoding='utf-8') as f:
                json.dump(self.chunk_metadata, f, ensure_ascii=False, indent=2)
            
            logger.info("FAISSç´¢å¼•æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"æ„å»ºFAISSç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def _load_documents(self) -> List[Dict[str, Any]]:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        documents = []
        
        # åŠ è½½MDæ–‡ä»¶
        md_file = Path("data/è¯¾æ ‡miner.md")
        if md_file.exists():
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
                documents.append({
                    'id': 'curriculum_standards',
                    'title': 'ä¹‰åŠ¡æ•™è‚²æ•°å­¦è¯¾ç¨‹æ ‡å‡†ï¼ˆ2022å¹´ç‰ˆï¼‰',
                    'content': content,
                    'type': 'curriculum',
                    'source': str(md_file)
                })
        
        # åŠ è½½çŸ¥è¯†å›¾è°±
        kg_file = Path("data/kowledge_graph.jsonl")
        if kg_file.exists():
            with open(kg_file, 'r', encoding='utf-8') as f:
                kg_data = []
                for line in f:
                    if line.strip():
                        try:
                            kg_data.append(json.loads(line.strip()))
                        except:
                            continue
                
                # å°†çŸ¥è¯†å›¾è°±è½¬æ¢ä¸ºæ–‡æ¡£
                for item in kg_data:
                    if 'name' in item and 'path' in item:
                        content = f"çŸ¥è¯†ç‚¹: {item['name']}\n"
                        if 'path' in item:
                            content += f"çŸ¥è¯†è·¯å¾„: {item['path']}\n"
                        if 'description' in item:
                            content += f"æè¿°: {item['description']}\n"
                        
                        documents.append({
                            'id': f"kg_{item.get('id', 'unknown')}",
                            'title': item['name'],
                            'content': content,
                            'type': 'knowledge_graph',
                            'source': 'knowledge_graph'
                        })
        
        # åŠ è½½é¢˜ç›®æ ·ä¾‹
        questions_file = Path("data/all_questions_filtered.jsonl")
        if questions_file.exists():
            with open(questions_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            q_data = json.loads(line.strip())
                            if 'content' in q_data and 'knowledge' in q_data:
                                content = f"é¢˜ç›®: {q_data['content']}\n"
                                content += f"çŸ¥è¯†ç‚¹: {q_data['knowledge']}\n"
                                if 'difficulty' in q_data:
                                    content += f"éš¾åº¦: {q_data['difficulty']}\n"
                                if 'grade' in q_data:
                                    content += f"å¹´çº§: {q_data['grade']}\n"
                                if 'competence' in q_data:
                                    content += f"ç´ å…»: {q_data['competence']}\n"
                                
                                documents.append({
                                    'id': f"q_{q_data.get('id', 'unknown')}",
                                    'title': f"é¢˜ç›®æ ·ä¾‹: {q_data['content'][:50]}...",
                                    'content': content,
                                    'type': 'question_sample',
                                    'source': 'question_samples'
                                })
                        except:
                            continue
        
        return documents
    
    def _split_documents(self, documents: List[Dict[str, Any]]) -> Tuple[List[str], List[Dict[str, Any]]]:
        """å°†æ–‡æ¡£åˆ†å‰²ä¸ºå—"""
        chunks = []
        metadata = []
        
        for doc in documents:
            content = doc['content']
            
            # æŒ‰æ®µè½åˆ†å‰²
            paragraphs = re.split(r'\n\s*\n', content)
            
            for para in paragraphs:
                para = para.strip()
                if len(para) < 50:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                    continue
                
                # å¦‚æœæ®µè½å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
                if len(para) > self.chunk_size:
                    sub_chunks = self._split_long_text(para)
                    for i, sub_chunk in enumerate(sub_chunks):
                        chunks.append(sub_chunk)
                        metadata.append({
                            'doc_id': doc['id'],
                            'doc_title': doc['title'],
                            'doc_type': doc['type'],
                            'source': doc['source'],
                            'chunk_index': i,
                            'total_chunks': len(sub_chunks)
                        })
                else:
                    chunks.append(para)
                    metadata.append({
                        'doc_id': doc['id'],
                        'doc_title': doc['title'],
                        'doc_type': doc['type'],
                        'source': doc['source'],
                        'chunk_index': 0,
                        'total_chunks': 1
                    })
        
        return chunks, metadata
    
    def _split_long_text(self, text: str) -> List[str]:
        """åˆ†å‰²é•¿æ–‡æœ¬"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk + sentence) > self.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                current_chunk += sentence + "ã€‚"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def search(self, query: str, top_k: int = 5, score_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.index is None:
            logger.error("FAISSç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode([query], convert_to_numpy=True)
            
            # æœç´¢
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            # æ„å»ºç»“æœ
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if score >= score_threshold:
                    results.append({
                        'content': self.chunks[idx],
                        'score': float(score),
                        'metadata': self.chunk_metadata[idx]
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"FAISSæœç´¢å¤±è´¥: {e}")
            return []
    
    def search_curriculum_standards(self, knowledge_point: str, difficulty: str, 
                                  core_competency: str, grade: str, grade_level: str) -> Dict[str, Any]:
        """
        åŸºäºFAISSæ£€ç´¢è¯¾ç¨‹æ ‡å‡†
        
        Args:
            knowledge_point: çŸ¥è¯†ç‚¹
            difficulty: éš¾åº¦
            core_competency: æ ¸å¿ƒç´ å…»
            grade: å¹´çº§
            grade_level: å¹´çº§çº§åˆ«
            
        Returns:
            è¯¾ç¨‹æ ‡å‡†æ£€ç´¢ç»“æœ
        """
        try:
            results = {
                "knowledge_point": knowledge_point,
                "difficulty": difficulty,
                "core_competency": core_competency,
                "grade": grade,
                "grade_level": grade_level,
                "curriculum_requirements": [],
                "teaching_suggestions": [],
                "learning_objectives": [],
                "assessment_criteria": []
            }
            
            # æ„å»ºå¤šä¸ªæŸ¥è¯¢
            queries = [
                f"{knowledge_point} {core_competency} æ•™å­¦è¦æ±‚ è¯¾ç¨‹æ ‡å‡†",
                f"{knowledge_point} {core_competency} å­¦ä¹ ç›®æ ‡ åŸ¹å…»",
                f"{grade} {knowledge_point} æ•°å­¦ æ•™è‚² æ ‡å‡†",
                f"{core_competency} ç´ å…» åŸ¹å…» æ•™å­¦å»ºè®®",
                f"{difficulty} {knowledge_point} è¯„ä»· è€ƒæ ¸ æ ‡å‡†",
                f"æ•°å­¦æ ¸å¿ƒç´ å…» {core_competency} å‘å±•",
                f"{knowledge_point} æ•°å­¦æ€ç»´ èƒ½åŠ› åŸ¹å…»"
            ]
            
            # æœç´¢å¹¶æ”¶é›†ç»“æœ
            all_results = []
            for query in queries:
                search_results = self.search(query, top_k=3, score_threshold=0.2)
                all_results.extend(search_results)
            
            # å»é‡
            seen_contents = set()
            unique_results = []
            for result in all_results:
                if result['content'] not in seen_contents:
                    seen_contents.add(result['content'])
                    unique_results.append(result)
            
            # æŒ‰åˆ†æ•°æ’åº
            unique_results.sort(key=lambda x: x['score'], reverse=True)
            
            # åˆ†ç±»ç»“æœ
            for result in unique_results[:15]:  # å–å‰15ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                content = result['content']
                score = result['score']
                metadata = result['metadata']
                
                # æ ¹æ®å†…å®¹ç‰¹å¾å’Œå…ƒæ•°æ®åˆ†ç±»
                if metadata.get('doc_type') == 'curriculum':
                    if any(word in content for word in ["è¦æ±‚", "æ ‡å‡†", "ç›®æ ‡", "æŒæ¡", "ç†è§£"]):
                        results["curriculum_requirements"].append(f"{content} (ç›¸ä¼¼åº¦: {score:.3f})")
                    elif any(word in content for word in ["å»ºè®®", "æ–¹æ³•", "ç­–ç•¥", "åŸ¹å…»", "å‘å±•"]):
                        results["teaching_suggestions"].append(f"{content} (ç›¸ä¼¼åº¦: {score:.3f})")
                    elif any(word in content for word in ["å­¦ä¹ ", "æŒæ¡", "ç†è§£", "èƒ½åŠ›", "ç´ å…»"]):
                        results["learning_objectives"].append(f"{content} (ç›¸ä¼¼åº¦: {score:.3f})")
                    elif any(word in content for word in ["è¯„ä»·", "è¯„ä¼°", "è€ƒæ ¸", "æµ‹è¯•", "æ£€æµ‹"]):
                        results["assessment_criteria"].append(f"{content} (ç›¸ä¼¼åº¦: {score:.3f})")
                    else:
                        results["curriculum_requirements"].append(f"{content} (ç›¸ä¼¼åº¦: {score:.3f})")
                elif metadata.get('doc_type') == 'question_sample':
                    # é¢˜ç›®æ ·ä¾‹å¯ä»¥ä½œä¸ºæ•™å­¦å»ºè®®çš„å‚è€ƒ
                    results["teaching_suggestions"].append(f"é¢˜ç›®æ ·ä¾‹: {content} (ç›¸ä¼¼åº¦: {score:.3f})")
                elif metadata.get('doc_type') == 'knowledge_graph':
                    # çŸ¥è¯†å›¾è°±ä¿¡æ¯å¯ä»¥ä½œä¸ºè¯¾ç¨‹è¦æ±‚
                    results["curriculum_requirements"].append(f"çŸ¥è¯†ç»“æ„: {content} (ç›¸ä¼¼åº¦: {score:.3f})")
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ•°é‡
            for key in results:
                if isinstance(results[key], list):
                    results[key] = results[key][:5]  # æœ€å¤š5æ¡
            
            logger.info(f"FAISSæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {sum(len(v) for v in results.values() if isinstance(v, list))} æ¡ç›¸å…³å†…å®¹")
            return results
            
        except Exception as e:
            logger.error(f"FAISSè¯¾ç¨‹æ ‡å‡†æ£€ç´¢å¤±è´¥: {e}")
            return {
                "knowledge_point": knowledge_point,
                "error": str(e)
            }
    
    def search_knowledge_graph(self, knowledge_point: str, grade: str = None, grade_level: str = None) -> List[Dict[str, Any]]:
        """åŸºäºFAISSæ£€ç´¢çŸ¥è¯†å›¾è°±"""
        query = f"{knowledge_point} çŸ¥è¯†å›¾è°± æ¦‚å¿µ å…³ç³»"
        if grade:
            query += f" {grade}"
        if grade_level:
            query += f" {grade_level}"
        
        results = self.search(query, top_k=10, score_threshold=0.2)
        
        # è¿‡æ»¤çŸ¥è¯†å›¾è°±ç›¸å…³ç»“æœ
        kg_results = []
        for result in results:
            if result['metadata'].get('doc_type') == 'knowledge_graph':
                kg_results.append({
                    'name': result['metadata'].get('doc_title', ''),
                    'content': result['content'],
                    'score': result['score']
                })
        
        return kg_results
    
    def get_question_samples(self, knowledge_point: str, difficulty: str, grade: str, core_competency: str) -> List[Dict[str, Any]]:
        """åŸºäºFAISSè·å–é¢˜ç›®æ ·ä¾‹"""
        query = f"{knowledge_point} {difficulty} {grade} {core_competency} é¢˜ç›®"
        
        results = self.search(query, top_k=5, score_threshold=0.2)
        
        # è¿‡æ»¤é¢˜ç›®æ ·ä¾‹
        question_results = []
        for result in results:
            if result['metadata'].get('doc_type') == 'question_sample':
                question_results.append({
                    'content': result['content'],
                    'score': result['score']
                })
        
        return question_results

def test_faiss_retriever():
    """æµ‹è¯•FAISSæ£€ç´¢å™¨"""
    print("ğŸ” æµ‹è¯•FAISSæ£€ç´¢å™¨...")
    
    try:
        retriever = FAISSRetriever()
        
        # æµ‹è¯•åŸºæœ¬æœç´¢
        print("\nğŸ“ æµ‹è¯•åŸºæœ¬æœç´¢:")
        results = retriever.search("è¿ç®—èƒ½åŠ› åŸ¹å…» æ•™å­¦", top_k=3)
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
        for i, result in enumerate(results, 1):
            print(f"  {i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
            print(f"     ç±»å‹: {result['metadata']['doc_type']}")
            print(f"     å†…å®¹: {result['content'][:100]}...")
        
        # æµ‹è¯•è¯¾ç¨‹æ ‡å‡†æ£€ç´¢
        print(f"\nğŸ“š æµ‹è¯•è¯¾ç¨‹æ ‡å‡†æ£€ç´¢:")
        curriculum_results = retriever.search_curriculum_standards(
            knowledge_point="ä¹˜æ³•åˆ†é…å¾‹",
            difficulty="æ˜“",
            core_competency="è¿ç®—èƒ½åŠ›",
            grade="å››å¹´çº§ä¸‹",
            grade_level="ä¸‹"
        )
        
        for key, values in curriculum_results.items():
            if isinstance(values, list) and values:
                print(f"   {key}: {len(values)} æ¡")
                for value in values[:2]:
                    print(f"     - {value[:80]}...")
        
        # æµ‹è¯•çŸ¥è¯†å›¾è°±æ£€ç´¢
        print(f"\nğŸ—ºï¸ æµ‹è¯•çŸ¥è¯†å›¾è°±æ£€ç´¢:")
        kg_results = retriever.search_knowledge_graph("ä¹˜æ³•åˆ†é…å¾‹", "å››å¹´çº§ä¸‹", "ä¸‹")
        print(f"æ‰¾åˆ° {len(kg_results)} ä¸ªçŸ¥è¯†å›¾è°±ç»“æœ")
        for result in kg_results[:2]:
            print(f"  - {result['name']}: {result['content'][:80]}...")
        
        print(f"\nâœ… FAISSæ£€ç´¢å™¨æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_faiss_retriever()
