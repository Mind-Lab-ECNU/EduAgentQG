#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æµ‹è¯„è„šæœ¬ - æ•´åˆå¤šæ ·æ€§ã€ä¸€è‡´æ€§å’ŒWin Rateæµ‹è¯„
"""

import json
import time
import re
import os
import sys
from datetime import datetime
from pathlib import Path
from openai import OpenAI, OpenAIError, AuthenticationError
from bert_score import score as bert_score
from sacrebleu.metrics import BLEU
from rouge_score import rouge_scorer
from nltk.translate.meteor_score import meteor_score as nltk_meteor
import jieba

# ==================== é…ç½® ====================

# OpenAI å®¢æˆ·ç«¯é…ç½®
client = OpenAI(
    api_key="sk-yiIIWRemdBaOfnzU9dxg3B5NRbaH5yf1lzNzG83MANDwoqy2",
    base_url="https://xiaoai.plus/v1"
)

# æµ‹è¯„å‚æ•°
MAX_RETRIES = 5
TIMEOUT = 300
model_name = ["deepseek-v3","gpt-4o"]  # å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„
MAX_WINRATE_QUESTIONS = None  # Win Rateæµ‹è¯„æœ€å¤§é¢˜ç›®æ•°é‡ï¼Œè®¾ä¸ºNoneè¡¨ç¤ºæµ‹è¯„æ‰€æœ‰é¢˜ç›®

# æµ‹è¯„å¼€å…³é…ç½®
EVALUATION_SWITCHES = {
    "diversity": False,      # å¤šæ ·æ€§æµ‹è¯„å¼€å…³
    "consistency": True,    # ä¸€è‡´æ€§æµ‹è¯„å¼€å…³
    "winrate": True        # Win Rateæµ‹è¯„å¼€å…³
}

# ç´¢å¼•èŒƒå›´é…ç½®ï¼ˆå¯é€‰ï¼‰
INDEX_RANGE = {
    "enabled": False,       # æ˜¯å¦å¯ç”¨ç´¢å¼•èŒƒå›´
    "start_index": 0,       # å¼€å§‹ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
    "end_index": 100         # ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰
}

# æ–‡ä»¶è·¯å¾„é…ç½®ï¼ˆé»˜è®¤å€¼ï¼Œå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
file1_path = r"D:\CODE\three_0921\data\choice_unquie_500.jsonl"  # é‡‘æ ‡å‡†æ•°æ®
file2_path = r"D:\CODE\three_0921\outputs\generated_questions_20250930_014833_writer_0-500_eval_V3é€‰æ‹©é¢˜ç”Ÿæˆ_deepseek_v3_input_1_20250930_041113.json"  # ç”Ÿæˆæ•°æ®1
file3_path = r"D:\CODE\three_0921\outputs\generated_questions_20250930_041116_writer_0-500_eval_V3é€‰æ‹©é¢˜ç”Ÿæˆ_deepseek_v3_input_2_20250930_041113.json"  # ç”Ÿæˆæ•°æ®2ï¼ˆå¤šæ ·æ€§æ¯”è¾ƒç”¨ï¼‰

# è¾“å‡ºé…ç½®
output_dir = Path(r"D:\CODE\three_0921\eval\unified_results")
output_dir.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# ==================== å·¥å…·å‡½æ•° ====================

def safe_json_parse(text: str):
    """å®‰å…¨çš„JSONè§£æï¼Œå…·æœ‰å¼ºå¤§çš„å®¹é”™èƒ½åŠ›"""
    if not text or not isinstance(text, str):
        return None
    
    # æ¸…ç†æ–‡æœ¬
    text = text.strip()
    if not text:
        return None
    
    # ç§»é™¤ä»£ç å—æ ‡è®°
    text = re.sub(r"```json\s*", "", text)
    text = re.sub(r"```\s*$", "", text)
    text = text.strip()
    
    # ç§»é™¤BOMå’Œé›¶å®½å­—ç¬¦
    text = text.replace("\ufeff", "").replace("\u200b", "").replace("\u200c", "").replace("\u200d", "")
    
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError as e:
        print(f"[safe_json_parse] ç›´æ¥è§£æå¤±è´¥: {e}")
        
        # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
        try:
            # 1. ç§»é™¤å°¾éšé€—å·
            text = re.sub(r',\s*}', '}', text)
            text = re.sub(r',\s*]', ']', text)
            
            # 2. ä¿®å¤å•å¼•å·ä¸ºåŒå¼•å·
            text = re.sub(r"'([^']*)':", r'"\1":', text)
            text = re.sub(r":\s*'([^']*)'", r': "\1"', text)
            
            # 3. ä¿®å¤True/False/null
            text = re.sub(r'\bTrue\b', 'true', text)
            text = re.sub(r'\bFalse\b', 'false', text)
            text = re.sub(r'\bNone\b', 'null', text)
            
            # 4. ç§»é™¤æ³¨é‡Š
            text = re.sub(r'//.*$', '', text, flags=re.MULTILINE)
            text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
            
            return json.loads(text)
        except json.JSONDecodeError as e2:
            print(f"[safe_json_parse] ä¿®å¤åè§£æå¤±è´¥: {e2}")
            
            # å°è¯•æå–JSONå¯¹è±¡
            try:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                brace_count = 0
                start_idx = -1
                end_idx = -1
                
                for i, char in enumerate(text):
                    if char == '{':
                        if start_idx == -1:
                            start_idx = i
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0 and start_idx != -1:
                            end_idx = i
                            break
                
                if start_idx != -1 and end_idx != -1:
                    json_text = text[start_idx:end_idx + 1]
                    return json.loads(json_text)
                else:
                    print(f"[safe_json_parse] æ— æ³•æ‰¾åˆ°å®Œæ•´çš„JSONå¯¹è±¡")
                    return None
                    
            except json.JSONDecodeError as e3:
                print(f"[safe_json_parse] æå–JSONå¯¹è±¡å¤±è´¥: {e3}")
                
                # æœ€åå°è¯•ï¼šé€è¡Œè§£æ
                try:
                    lines = text.split('\n')
                    json_lines = []
                    in_json = False
                    
                    for line in lines:
                        line = line.strip()
                        if line.startswith('{') or in_json:
                            in_json = True
                            json_lines.append(line)
                            if line.endswith('}') and line.count('{') == line.count('}'):
                                break
                    
                    if json_lines:
                        json_text = '\n'.join(json_lines)
                        return json.loads(json_text)
                    else:
                        print(f"[safe_json_parse] æ— æ³•ä»æ–‡æœ¬ä¸­æå–JSON")
                        return None
                        
                except json.JSONDecodeError as e4:
                    print(f"[safe_json_parse] æ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥äº†: {e4}")
                    print(f"[safe_json_parse] åŸå§‹æ–‡æœ¬å‰200å­—ç¬¦: {text[:200]}")
                    return None

def load_data_with_format_check(file_path):
    """åŠ è½½æ•°æ®å¹¶å¤„ç†ä¸åŒæ ¼å¼"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
        print(f"ğŸ“ å°è¯•ä¿®å¤æ–‡ä»¶: {file_path}")
        
        # å°è¯•ä¿®å¤JSONæ ¼å¼
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            if file_path.endswith('.jsonl') or ('\n' in content and content.count('{') > 1):
                print("ğŸ”§ æ£€æµ‹åˆ°JSONLæ ¼å¼ï¼Œè½¬æ¢ä¸ºJSONæ•°ç»„...")
                lines = content.strip().split('\n')
                json_objects = []
                
                for i, line in enumerate(lines):
                    line = line.strip()
                    if line:
                        try:
                            obj = json.loads(line)
                            json_objects.append(obj)
                        except json.JSONDecodeError as line_e:
                            print(f"âš ï¸ è·³è¿‡ç¬¬{i+1}è¡Œ: {line_e}")
                            continue
                
                if json_objects:
                    data = json_objects
                    print(f"âœ… è½¬æ¢æˆåŠŸï¼Œæå–åˆ° {len(json_objects)} ä¸ªJSONå¯¹è±¡")
                else:
                    raise ValueError("æ— æ³•è§£æJSONLå†…å®¹")
            
            # å°è¯•å¤„ç†å¤šä¸ªJSONå¯¹è±¡çš„æƒ…å†µ
            elif content.strip().startswith('{') and content.count('{') > 1:
                print("ğŸ”§ æ£€æµ‹åˆ°å¤šä¸ªJSONå¯¹è±¡ï¼Œå°è¯•åŒ…è£…æˆæ•°ç»„...")
                lines = content.strip().split('\n')
                json_objects = []
                for line in lines:
                    line = line.strip()
                    if line and (line.startswith('{') or line.startswith('[')):
                        try:
                            obj = json.loads(line)
                            json_objects.append(obj)
                        except:
                            continue
                
                if json_objects:
                    data = json_objects
                    print(f"âœ… ä¿®å¤æˆåŠŸï¼Œæå–åˆ° {len(json_objects)} ä¸ªJSONå¯¹è±¡")
                else:
                    raise ValueError("æ— æ³•è§£æJSONå†…å®¹")
            else:
                raise ValueError("æ— æ³•ä¿®å¤JSONæ ¼å¼")
                
        except Exception as e2:
            print(f"âŒ ä¿®å¤å¤±è´¥: {e2}")
            raise ValueError(f"æ— æ³•åŠ è½½æ–‡ä»¶ {file_path}: {e}")
    
    # å¤„ç†æ–°çš„æ–‡ä»¶æ ¼å¼ï¼Œæå– results å­—æ®µ
    if isinstance(data, dict) and "results" in data:
        items = data["results"]
        print(f"ğŸ“ æ£€æµ‹åˆ°æ–°æ ¼å¼ï¼Œæå– {len(items)} æ¡ç»“æœ")
    elif isinstance(data, list):
        items = data
        print(f"ğŸ“ æ£€æµ‹åˆ°æ—§æ ¼å¼ï¼ŒåŒ…å« {len(items)} æ¡ç»“æœ")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
    
    return items

def apply_index_range(data, data_name="æ•°æ®"):
    """æ ¹æ®ç´¢å¼•èŒƒå›´è¿‡æ»¤æ•°æ®"""
    if not INDEX_RANGE["enabled"]:
        return data
    
    start_idx = INDEX_RANGE["start_index"]
    end_idx = INDEX_RANGE["end_index"]
    
    if start_idx < 0 or end_idx <= start_idx:
        print(f"âš ï¸ ç´¢å¼•èŒƒå›´æ— æ•ˆ: {start_idx}-{end_idx}ï¼Œè·³è¿‡è¿‡æ»¤")
        return data
    
    if end_idx > len(data):
        end_idx = len(data)
        print(f"âš ï¸ ç»“æŸç´¢å¼•è¶…å‡ºæ•°æ®èŒƒå›´ï¼Œè°ƒæ•´ä¸º: {start_idx}-{end_idx}")
    
    filtered_data = data[start_idx:end_idx]
    print(f"ğŸ”§ {data_name}ç´¢å¼•è¿‡æ»¤: {len(data)} -> {len(filtered_data)} (ç´¢å¼• {start_idx}-{end_idx-1})")
    
    return filtered_data

def tokenize_text(text, mode="jieba"):
    """æ–‡æœ¬åˆ†è¯"""
    if mode == "jieba":
        return list(jieba.cut(text))
    else:  # char
        return list(text)

# ==================== å¤šæ ·æ€§æµ‹è¯„ ====================

def calculate_diversity_metrics(file2_data, file3_data):
    """è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡ - æ¯”è¾ƒç›¸åŒque_idçš„é¢˜ç›®ï¼ˆä½¿ç”¨å¤šç§æŒ‡æ ‡ï¼‰"""
    print("\nğŸ” å¼€å§‹å¤šæ ·æ€§æµ‹è¯„...")
    
    # æ„å»ºque_idåˆ°é¢˜ç›®çš„æ˜ å°„
    print("ğŸ“ æ„å»ºé¢˜ç›®æ˜ å°„...")
    data2_dict = {}
    data3_dict = {}
    
    for item in file2_data:
        que_id = item.get("que_id", "")
        question_data = item.get("question", {})
        
        # å¤„ç†V3æ ¼å¼ï¼ˆquestionæ˜¯å­—ç¬¦ä¸²ï¼‰å’Œbaselineæ ¼å¼ï¼ˆquestionæ˜¯å¯¹è±¡ï¼‰
        if isinstance(question_data, str):
            # V3æ ¼å¼ï¼šquestionæ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«é¢˜ç›®å’Œé€‰é¡¹
            if que_id and question_data:
                data2_dict[que_id] = f"é¢˜ç›®ï¼š{question_data}"
        elif isinstance(question_data, dict):
            # baselineæ ¼å¼ï¼šquestionæ˜¯å¯¹è±¡ï¼ŒåŒ…å«questionå’Œoptionså­—æ®µ
            q = question_data.get("question", "")
            opts = question_data.get("options", [])
            if que_id and q:
                if opts:
                    options_str = "ï¼›".join(opts)
                    text = f"é¢˜ç›®ï¼š{q}\né€‰é¡¹ï¼š{options_str}"
                else:
                    text = f"é¢˜ç›®ï¼š{q}"
                data2_dict[que_id] = text
    
    for item in file3_data:
        que_id = item.get("que_id", "")
        question_data = item.get("question", {})
        
        # å¤„ç†V3æ ¼å¼ï¼ˆquestionæ˜¯å­—ç¬¦ä¸²ï¼‰å’Œbaselineæ ¼å¼ï¼ˆquestionæ˜¯å¯¹è±¡ï¼‰
        if isinstance(question_data, str):
            # V3æ ¼å¼ï¼šquestionæ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«é¢˜ç›®å’Œé€‰é¡¹
            if que_id and question_data:
                data3_dict[que_id] = f"é¢˜ç›®ï¼š{question_data}"
        elif isinstance(question_data, dict):
            # baselineæ ¼å¼ï¼šquestionæ˜¯å¯¹è±¡ï¼ŒåŒ…å«questionå’Œoptionså­—æ®µ
            q = question_data.get("question", "")
            opts = question_data.get("options", [])
            if que_id and q:
                if opts:
                    options_str = "ï¼›".join(opts)
                    text = f"é¢˜ç›®ï¼š{q}\né€‰é¡¹ï¼š{options_str}"
                else:
                    text = f"é¢˜ç›®ï¼š{q}"
                data3_dict[que_id] = text
    
    print(f"ğŸ“Š æ–‡ä»¶2é¢˜ç›®æ•°: {len(data2_dict)}, æ–‡ä»¶3é¢˜ç›®æ•°: {len(data3_dict)}")
    
    # æ‰¾åˆ°å…±åŒçš„que_id
    common_ids = set(data2_dict.keys()) & set(data3_dict.keys())
    print(f"ğŸ“Š å…±åŒque_idæ•°é‡: {len(common_ids)}")
    
    if not common_ids:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°å…±åŒçš„que_id"}
    
    # å‡†å¤‡æ¯”è¾ƒæ•°æ®
    refs = [data2_dict[qid] for qid in common_ids]
    cands = [data3_dict[qid] for qid in common_ids]
    
    print(f"ğŸ”§ æ¯”è¾ƒæ‰€æœ‰å…±åŒé¢˜ç›®: {len(common_ids)} ä¸ª")
    print(f"   â±ï¸ é¢„è®¡è€—æ—¶: {len(common_ids) * 0.5}ç§’")
    
    # è®¡ç®—å¤šç§å¤šæ ·æ€§æŒ‡æ ‡
    print("\nğŸ“Š è®¡ç®—å¤šæ ·æ€§æŒ‡æ ‡...")
    print("   â³ æ­£åœ¨åŠ è½½BERTæ¨¡å‹ï¼Œè¯·ç¨å€™...")
    
    try:
        # BERTScore
        print("   ğŸ”„ è®¡ç®—BERTScore...")
        P, R, F1 = bert_score(cands, refs, lang="zh", verbose=False)
        avg_bert_f1 = F1.mean().item()
        print(f"   âœ… BERTScoreå®Œæˆ: {avg_bert_f1:.4f}")
        
        # BLEU
        print("   ğŸ”„ è®¡ç®—BLEU...")
        from sacrebleu.metrics import BLEU
        bleu_metric = BLEU()
        cands_tok = [" ".join(jieba.cut(t)) for t in cands]
        refs_tok = [[" ".join(jieba.cut(t)) for t in refs]]
        bleu = bleu_metric.corpus_score(cands_tok, refs_tok).score
        print(f"   âœ… BLEUå®Œæˆ: {bleu:.4f}")
        
        # METEOR
        print("   ğŸ”„ è®¡ç®—METEOR...")
        from nltk.translate.meteor_score import meteor_score as nltk_meteor
        meteor_scores = []
        for r, c in zip(refs, cands):
            r_tok = list(jieba.cut(r))
            c_tok = list(jieba.cut(c))
            meteor_scores.append(nltk_meteor([r_tok], c_tok))
        avg_meteor = sum(meteor_scores) / len(meteor_scores)
        print(f"   âœ… METEORå®Œæˆ: {avg_meteor:.4f}")
        
        # ROUGE-L
        print("   ğŸ”„ è®¡ç®—ROUGE-L...")
        from rouge_score import rouge_scorer
        rouge = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
        rouge_scores = []
        for r, c in zip(refs, cands):
            r_tok = "".join(jieba.cut(r))
            c_tok = "".join(jieba.cut(c))
            rouge_scores.append(rouge.score(r_tok, c_tok)["rougeL"].fmeasure)
        avg_rougeL = sum(rouge_scores) / len(rouge_scores)
        print(f"   âœ… ROUGE-Lå®Œæˆ: {avg_rougeL:.4f}")
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        diversity_metrics = {
            "avg_BLEU": bleu,
            "avg_METEOR": avg_meteor,
            "avg_ROUGE_L": avg_rougeL,
            "avg_BERTScore_F1": avg_bert_f1,
            "matched_question_count": len(common_ids),
            "tokenize_mode": "jieba",
            "evaluation_model": "bert_score+sacrebleu+nltk+rouge_score"  # å¤šæ ·æ€§ä½¿ç”¨å¤šç§å·¥å…·
        }
        
        print(f"\nâœ… å¤šæ ·æ€§æµ‹è¯„å®Œæˆ!")
        print(f"   ğŸ“Š BLEU: {bleu:.4f}")
        print(f"   ğŸ“Š METEOR: {avg_meteor:.4f}")
        print(f"   ğŸ“Š ROUGE-L: {avg_rougeL:.4f}")
        print(f"   ğŸ“Š BERTScore-F1: {avg_bert_f1:.4f}")
        print(f"   ğŸ“Š æ¯”è¾ƒé¢˜ç›®æ•°: {len(common_ids)}")
        
        return diversity_metrics
        
    except Exception as e:
        print(f"   âŒ å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {e}")
        return {"error": f"å¤šæ ·æ€§è®¡ç®—å¤±è´¥: {str(e)}"}

# ==================== ä¸€è‡´æ€§æµ‹è¯„ ====================

def calculate_consistency_metrics(file2_data, model_name=None):
    """è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡"""
    print("\nğŸ¯ å¼€å§‹ä¸€è‡´æ€§æµ‹è¯„...")
    
    # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…¨å±€å˜é‡
    current_model = model_name if model_name else globals().get('model_name', 'gpt-4o')
    
    results = []
    total_questions = len(file2_data)
    
    for i, item in enumerate(file2_data, 1):
        if "question" not in item:
            continue
            
        question_data = item["question"]
        
        # å¤„ç†V3æ ¼å¼ï¼ˆquestionæ˜¯å­—ç¬¦ä¸²ï¼‰å’Œbaselineæ ¼å¼ï¼ˆquestionæ˜¯å¯¹è±¡ï¼‰
        if isinstance(question_data, str):
            q = question_data
        elif isinstance(question_data, dict):
            q = question_data.get("question", "")
        else:
            continue
            
        if not q:
            continue
            
        # æ„å»ºæ•™è‚²ç›®æ ‡
        input_data = item.get("input", {})
        education_goals = {
            "grade": input_data.get("grade", ""),
            "difficulty": input_data.get("difficulty", ""),
            "competence": input_data.get("competence", []),
            "knowledge": input_data.get("knowledge", ""),
            "question_type": input_data.get("question_type", "")
        }
        
        prompt = f"""
å°†ä»¥ä¸‹ä»»åŠ¡è§†ä¸ºæ•°å­¦æ•™è‚²è¯„ä¼°ä½œä¸šã€‚ä½ å°†ä½œä¸ºä¸€åé«˜çº§æ•°å­¦æ•™è‚²ä¸“å®¶ï¼Œä¸¥æ ¼è¯„ä¼°ç»™å®šçš„æ•°å­¦é—®é¢˜åŠå…¶ç›¸å…³çš„æ•™è‚²ç›®æ ‡ã€‚ä½ å°†æ ¹æ®ä»¥ä¸‹ä¸‰ä¸ªå…³é”®ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œä»”ç»†æ£€æŸ¥æ¯ä¸€é¡¹ã€‚ç»™å‡º0-10ä¹‹é—´çš„è¯„åˆ†ï¼Œå¯ä»¥æœ‰ä¸€ä½å°æ•°ï¼Œæ¯”å¦‚8.2ï¼š
å…³é”®è¯„ä¼°ç»´åº¦ï¼š
1. çŸ¥è¯†ç‚¹è¦†ç›–ï¼šéªŒè¯æ‰€æœ‰å¿…éœ€æ¦‚å¿µçš„å®Œæ•´è¦†ç›–ï¼Œä»»ä½•é—æ¼æˆ–å¼•å…¥æœªæåŠçš„ç‚¹éƒ½æ˜¯ä¸åˆè§„çš„
2. éš¾åº¦é€‚é…æ€§ï¼šåˆ†æé¢˜ç›®çš„éš¾åº¦æ°´å¹³æ˜¯å¦ä¸æ•™è‚²ç›®æ ‡ä¸€è‡´
3. é¢˜ç›®å‡†ç¡®ç‡ï¼šé¢˜ç›®æ˜¯å¦å¯è§£ï¼Œç»™å‡ºçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
4. ç´ å…»å¯¼å‘æ€§ï¼šåˆ†æé¢˜ç›®å¯¹ç´ å…»çš„åŸ¹å…»æ˜¯å¦ä¸æ•™è‚²ç›®æ ‡ä¸€è‡´
ä¸¥æ ¼è¦æ±‚ï¼šåœ¨æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹ä¸­ä¿æŒå®¢è§‚æ€§å’Œä¸¥è°¨æ€§

é¢˜ç›®: {q}
æ•™è‚²ç›®æ ‡: {json.dumps(education_goals, ensure_ascii=False)}

è¯·ä¸¥æ ¼è¾“å‡º JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•æ–‡å­—æˆ–ä»£ç å—ï¼Œæ ¼å¼ï¼š
{{
    "que_id": "{item.get('que_id', '')}",
    "è¯„æµ‹ç»“æœ": {{
        "çŸ¥è¯†ç‚¹åŒ¹é…åº¦": {{"score": 0.0, "reason": ""}},
        "éš¾åº¦é€‚é…æ€§": {{"score": 0.0, "reason": ""}},
        "é¢˜ç›®å‡†ç¡®æ€§": {{"score": 0.0, "reason": ""}},
        "ç´ å…»å¯¼å‘æ€§": {{"score": 0.0, "reason": ""}}
    }},
    "Education_Goals": {json.dumps(education_goals, ensure_ascii=False)},
    "Question": {json.dumps(q, ensure_ascii=False)}
}}
"""
        
        for attempt in range(MAX_RETRIES):
            try:
                response = client.chat.completions.create(
                    model=current_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    timeout=TIMEOUT
                )
                
                result_text = response.choices[0].message.content
                result_json = safe_json_parse(result_text)
                
                if result_json is not None:
                    # ä¿ç•™å°æ•°ä¸€ä½
                    for dim in ["çŸ¥è¯†ç‚¹åŒ¹é…åº¦", "éš¾åº¦é€‚é…æ€§", "é¢˜ç›®å‡†ç¡®æ€§", "ç´ å…»å¯¼å‘æ€§"]:
                        if dim in result_json.get("è¯„æµ‹ç»“æœ", {}) and "score" in result_json["è¯„æµ‹ç»“æœ"][dim]:
                            score_val = result_json["è¯„æµ‹ç»“æœ"][dim]["score"]
                            result_json["è¯„æµ‹ç»“æœ"][dim]["score"] = round(float(score_val), 1)
                    results.append(result_json)
                break
                
            except (AuthenticationError, OpenAIError, Exception) as e:
                print(f"[{item.get('que_id', '')}] è°ƒç”¨å‡ºé”™: {e}, å°è¯• {attempt+1}/{MAX_RETRIES}")
                time.sleep(3)
        
        if i % 10 == 0:
            print(f"ğŸ“Š ä¸€è‡´æ€§æµ‹è¯„è¿›åº¦: {i}/{total_questions}")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    if results:
        stats = {}
        for dim in ["çŸ¥è¯†ç‚¹åŒ¹é…åº¦", "éš¾åº¦é€‚é…æ€§", "é¢˜ç›®å‡†ç¡®æ€§", "ç´ å…»å¯¼å‘æ€§"]:
            valid_scores = [
                r["è¯„æµ‹ç»“æœ"][dim]["score"] for r in results
                if isinstance(r, dict)
                and isinstance(r.get("è¯„æµ‹ç»“æœ"), dict)
                and isinstance(r["è¯„æµ‹ç»“æœ"].get(dim), dict)
                and isinstance(r["è¯„æµ‹ç»“æœ"][dim].get("score"), (int, float))
            ]
            if valid_scores:
                stats[dim] = round(sum(valid_scores) / len(valid_scores), 1)
            else:
                stats[dim] = 0.0
        
        stats["æ€»å¹³å‡åˆ†"] = round(sum(stats[dim] for dim in ["çŸ¥è¯†ç‚¹åŒ¹é…åº¦", "éš¾åº¦é€‚é…æ€§", "é¢˜ç›®å‡†ç¡®æ€§", "ç´ å…»å¯¼å‘æ€§"]) / 4, 1)
        
        consistency_metrics = {
            "statistics": stats,
            "total_evaluated": len(results),
            "total_questions": total_questions,
            "evaluation_model": current_model,
            "detailed_results": results  # æ·»åŠ è¯¦ç»†çš„é¢˜ç›®ç»“æœ
        }
        
        print(f"âœ… ä¸€è‡´æ€§æµ‹è¯„å®Œæˆ - æ€»å¹³å‡åˆ†: {stats['æ€»å¹³å‡åˆ†']}")
        return consistency_metrics
    
    return {"error": "æ— æ³•å®Œæˆä¸€è‡´æ€§æµ‹è¯„"}

# ==================== Win Rateæµ‹è¯„ ====================

def calculate_winrate_metrics(file1_data, file2_data, model_name=None):
    """è®¡ç®—Win RateæŒ‡æ ‡"""
    print("\nğŸ† å¼€å§‹Win Rateæµ‹è¯„...")
    
    # ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å…¨å±€å˜é‡
    current_model = model_name if model_name else globals().get('model_name', 'gpt-4o')
    
    # è·å–å…¨å±€çš„MAX_WINRATE_QUESTIONSå‚æ•°
    max_questions = globals().get('MAX_WINRATE_QUESTIONS', 50)
    
    # æ„å»ºæ•°æ®å­—å…¸
    data1 = {item["que_id"]: item for item in file1_data}
    data2 = {item["que_id"]: item for item in file2_data}
    
    # æ‰¾åˆ°å…±åŒé¢˜ç›®
    common_ids = set(data1.keys()) & set(data2.keys())
    if not common_ids:
        return {"error": "æ²¡æœ‰åŒ¹é…åˆ°ç›¸åŒçš„ que_id"}
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(common_ids)} é“å…±åŒé¢˜ç›®")
    
    # é€‰æ‹©æµ‹è¯„é¢˜ç›®
    if max_questions is None:
        selected_ids = list(common_ids)
        print(f"ğŸ“Š å°†æµ‹è¯„æ‰€æœ‰ {len(selected_ids)} é“é¢˜ç›®")
    else:
        selected_ids = list(common_ids)[:max_questions]
        print(f"ğŸ“Š å°†æµ‹è¯„å‰ {len(selected_ids)} é“é¢˜ç›®ï¼ˆé™åˆ¶ï¼š{max_questions}é“ï¼‰")
    
    results = []
    for i, que_id in enumerate(selected_ids, 1):
        item1 = data1[que_id]
        item2 = data2[que_id]
        
        # æ„å»ºé¢˜ç›®æ–‡æœ¬
        def build_question_text(item):
            # æ£€æŸ¥æ•°æ®ç»“æ„
            if "question" in item:
                question_data = item["question"]
                if isinstance(question_data, str):
                    # V3æ ¼å¼ï¼šquestionæ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…å«é¢˜ç›®å’Œé€‰é¡¹
                    return f"é¢˜ç›®ï¼š{question_data}"
                elif isinstance(question_data, dict):
                    # baselineæ ¼å¼ï¼šåµŒå¥—çš„questionå­—æ®µ
                    q = question_data.get("question", "")
                    opts = question_data.get("options", [])
                    a = question_data.get("answer", "")
                    if opts:
                        options_str = "ï¼›".join(opts)
                        return f"é¢˜ç›®ï¼š{q}\né€‰é¡¹ï¼š{options_str}\nç­”æ¡ˆï¼š{a}"
                    else:
                        return f"é¢˜ç›®ï¼š{q}\nç­”æ¡ˆï¼š{a}"
            elif "content" in item:
                # file1_data çš„æ ¼å¼ï¼šç›´æ¥çš„contentå’Œanswerå­—æ®µ
                q = item.get("content", "")
                opts = []  # åŸå§‹æ•°æ®ä¸­é€‰é¡¹åœ¨contentä¸­
                a = item.get("answer", "")
                return f"é¢˜ç›®ï¼š{q}\nç­”æ¡ˆï¼š{a}"
            else:
                # å…¶ä»–æ ¼å¼çš„å…¼å®¹å¤„ç†
                q = item.get("question", "")
                opts = item.get("options", [])
                a = item.get("answer", "")
                if opts:
                    options_str = "ï¼›".join(opts)
                    return f"é¢˜ç›®ï¼š{q}\né€‰é¡¹ï¼š{options_str}\nç­”æ¡ˆï¼š{a}"
                else:
                    return f"é¢˜ç›®ï¼š{q}\nç­”æ¡ˆï¼š{a}"
        
        question1 = build_question_text(item1)
        question2 = build_question_text(item2)
        
        # ä»ç”Ÿæˆé¢˜ç›®ä¸­å–æ•™è‚²ç›®æ ‡ï¼ˆè‹¥å­˜åœ¨ï¼‰
        input_data = item2.get("input", {}) if isinstance(item2, dict) else {}
        education_goals = {
            "grade": input_data.get("grade", ""),
            "difficulty": input_data.get("difficulty", ""),
            "competence": input_data.get("competence", []),
            "knowledge": input_data.get("knowledge", []),
            "question_type": input_data.get("question_type", "")
        }

        prompt = f"""
ä½œä¸ºä¸€åèµ„æ·±æ•°å­¦æ•™è‚²ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œä¸¥æ ¼çš„è¯„ä»·å’Œæ¯”è¾ƒã€‚åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®ä»¥ä¸‹ç»´åº¦å¯¹æ¯ä¸ªé—®é¢˜è¿›è¡Œåˆ†æï¼Œå¹¶ç¡®å®šå“ªä¸ªé—®é¢˜æ›´ç¬¦åˆæ•™è‚²ç›®æ ‡ã€‚
è¯„ä¼°ç»´åº¦ï¼š
1. æ¦‚å¿µè¦†ç›–çš„å®Œå¤‡æ€§ï¼šåˆ†ææ‰€éœ€æ¦‚å¿µçš„è¦†ç›–é¢ï¼Œæ£€æŸ¥ç¼ºå¤±æˆ–å†—ä½™ç‚¹
2. éš¾åº¦æ°´å¹³çš„åŒ¹é…ï¼šåˆ†æé¢˜ç›®æ˜¯å¦æ»¡è¶³æ•™è‚²ç›®æ ‡çš„éš¾åº¦æ°´å¹³
3. ä¸èƒ½åŠ›å‘å±•çš„ç›¸å…³æ€§ï¼šç¡®è®¤ç¬¦åˆè¯¥å¹´çº§å­¦ç”Ÿçš„æ°´å¹³ï¼Œä¸æ¦‚è¿°çš„è¦æ±‚ä¸€è‡´
4. æ•°å­¦ç´ å…»çš„å‘å±•ï¼šåˆ†æå¯¹æ•°å­¦ç´ å…»å‘å±•çš„è´¡çŒ®
5. ç»“æ„çš„ç§‘å­¦è®¾è®¡ï¼šè¯„ä¼°é—®é¢˜ç»“æ„åˆç†æ€§ï¼Œç»„ç»‡å’ŒæŒ‡å¯¼è´¨é‡
6. æ–‡æœ¬æ¸…æ™°åº¦å’Œè¿è´¯æ€§ï¼šè¯„ä¼°æªè¾æ¸…æ™°åº¦å’Œç®€æ˜æ€§ï¼Œæœ‰æ•ˆäº¤æµè§£é¢˜ä¿¡æ¯
7. é¢˜ç›®å‡†ç¡®ç‡ï¼šé¢˜ç›®æ˜¯å¦å¯è§£ï¼Œç»™å‡ºçš„ç­”æ¡ˆæ˜¯å¦æ­£ç¡®

è¾“å‡ºæ ¼å¼ï¼š{{"Better_Quest": 1æˆ–2, "åŸå› ": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±ï¼Œè§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹©çš„é—®é¢˜æ›´å¥½ï¼Œå¹¶è¯´æ˜å“ªä¸ªç»´åº¦(S)è¡¨ç°æ›´å¥½ã€‚"}}
å¿…è¾“é¡¹ï¼šEducation_Goalsï¼š{json.dumps(education_goals, ensure_ascii=False)}
é—®é¢˜å¯¹ï¼š
é—®é¢˜1ï¼š
{question1}

é—®é¢˜2ï¼š
{question2}
"""
        
        for attempt in range(MAX_RETRIES):
            try:
                response = client.chat.completions.create(
                    model=current_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3,
                    timeout=TIMEOUT
                )
                
                result_text = response.choices[0].message.content
                result_json = safe_json_parse(result_text)

                winner_val = None
                reason_val = ""
                scores_val = {}
                if isinstance(result_json, dict):
                    if "winner" in result_json:
                        winner_val = result_json.get("winner")
                        reason_val = result_json.get("reason", "")
                        scores_val = result_json.get("scores", {})
                    elif "Better_Quest" in result_json or "better_quest" in result_json:
                        bq = result_json.get("Better_Quest", result_json.get("better_quest"))
                        if bq in [1, 2, "1", "2"]:
                            winner_val = "A" if str(bq) == "1" else "B"
                        reason_val = result_json.get("åŸå› ", result_json.get("reason", ""))

                if winner_val in ["A", "B"]:
                    results.append({
                        "que_id": que_id,
                        "winner": winner_val,
                        "reason": reason_val,
                        "scores": scores_val,
                        "question_A": question1,
                        "question_B": question2
                    })
                    break
                    
            except (AuthenticationError, OpenAIError, Exception) as e:
                print(f"[{que_id}] Win Rateæµ‹è¯„å‡ºé”™: {e}, å°è¯• {attempt+1}/{MAX_RETRIES}")
                time.sleep(3)
        
        if i % 10 == 0:
            print(f"ğŸ“Š Win Rateæµ‹è¯„è¿›åº¦: {i}/{len(selected_ids)}")
    
    # è®¡ç®—Win Rate
    if results:
        wins = sum(1 for r in results if r["winner"] == "B")  # Bæ˜¯ç”Ÿæˆé¢˜ç›®
        total = len(results)
        win_rate = wins / total if total > 0 else 0
        
        winrate_metrics = {
            "win_rate": round(win_rate, 4),
            "wins": wins,
            "total": total,
            "evaluated_questions": results,
            "evaluation_model": current_model
        }
        
        print(f"âœ… Win Rateæµ‹è¯„å®Œæˆ - Win Rate: {win_rate:.4f} ({wins}/{total})")
        return winrate_metrics
    
    return {"error": "æ— æ³•å®ŒæˆWin Rateæµ‹è¯„"}

# ==================== ä¸»å‡½æ•° ====================

def main(file1=None, file2=None, file3=None, eval_model=None, start_idx=None, end_idx=None, max_winrate_questions=None):
    """ä¸»æµ‹è¯„å‡½æ•°"""
    global file1_path, file2_path, file3_path, model_name, INDEX_RANGE, MAX_WINRATE_QUESTIONS
    
    # å¦‚æœä¼ å…¥äº†å‚æ•°ï¼Œåˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
    if file1:
        file1_path = file1
    if file2:
        file2_path = file2
    if file3:
        file3_path = file3
    if eval_model:
        model_name = eval_model
    if start_idx is not None and end_idx is not None:
        INDEX_RANGE["enabled"] = True
        INDEX_RANGE["start_index"] = start_idx
        INDEX_RANGE["end_index"] = end_idx
    if max_winrate_questions is not None:
        MAX_WINRATE_QUESTIONS = max_winrate_questions
    
    # å¤„ç†æ¨¡å‹åç§°ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æˆ–æ•°ç»„ï¼‰
    if isinstance(model_name, str):
        models = [model_name]
    elif isinstance(model_name, list):
        models = model_name
    else:
        models = [str(model_name)]
    
    print("ğŸš€ å¼€å§‹ç»Ÿä¸€æµ‹è¯„...")
    print(f"ğŸ“ å‚è€ƒæ•°æ®: {file1_path}")
    print(f"ğŸ“ ç”Ÿæˆæ•°æ®: {file2_path}")
    if file3_path:
        print(f"ğŸ“ ç”Ÿæˆæ•°æ®2: {file3_path}")
    print(f"ğŸ¤– æµ‹è¯„æ¨¡å‹: {model_name}")
    print("=" * 60)
    
    try:
        # åŠ è½½æ•°æ®
        print("ğŸ“– åŠ è½½æ•°æ®...")
        file1_data = load_data_with_format_check(file1_path)
        file2_data = load_data_with_format_check(file2_path)
        
        # åŠ è½½ç¬¬ä¸‰ä¸ªæ–‡ä»¶ï¼ˆå¤šæ ·æ€§æ¯”è¾ƒç”¨ï¼‰
        print("ğŸ“– åŠ è½½ç¬¬ä¸‰ä¸ªæ–‡ä»¶ï¼ˆå¤šæ ·æ€§æ¯”è¾ƒç”¨ï¼‰...")
        file3_data = load_data_with_format_check(file3_path)
        
        # åº”ç”¨ç´¢å¼•èŒƒå›´è¿‡æ»¤
        if INDEX_RANGE["enabled"]:
            print(f"\nğŸ”§ åº”ç”¨ç´¢å¼•èŒƒå›´è¿‡æ»¤: {INDEX_RANGE['start_index']}-{INDEX_RANGE['end_index']-1}")
            file1_data = apply_index_range(file1_data, "é‡‘æ ‡å‡†æ•°æ®")
            file2_data = apply_index_range(file2_data, "ç”Ÿæˆæ•°æ®1")
            file3_data = apply_index_range(file3_data, "ç”Ÿæˆæ•°æ®2")
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ†åˆ«æ‰§è¡Œæµ‹è¯„
        all_results = {}
        
        for model_idx, current_model in enumerate(models):
            print(f"\n{'='*60}")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ {model_idx + 1}/{len(models)}: {current_model}")
            print(f"{'='*60}")
            
            # è®¾ç½®å½“å‰æ¨¡å‹ï¼ˆä½¿ç”¨å±€éƒ¨å˜é‡ï¼‰
            current_model_name = current_model
            
            # æ‰§è¡Œå„é¡¹æµ‹è¯„
            evaluation_results = {
                "timestamp": timestamp,
                "model_name": current_model,
                "evaluation_model": current_model,  # ç”¨äºè¯„ä»·çš„æ¨¡å‹
                "file1_path": file1_path,  # é‡‘æ ‡å‡†
                "file2_path": file2_path,  # ç”Ÿæˆæ•°æ®1
                "file3_path": file3_path,  # ç”Ÿæˆæ•°æ®2
                "file1_count": len(file1_data),
                "file2_count": len(file2_data),
                "file3_count": len(file3_data),
                "index_range": INDEX_RANGE if INDEX_RANGE["enabled"] else None,  # ç´¢å¼•èŒƒå›´ä¿¡æ¯
                "evaluation_switches": EVALUATION_SWITCHES  # æµ‹è¯„å¼€å…³çŠ¶æ€
            }
            
            # æ ¹æ®å¼€å…³æ‰§è¡Œæµ‹è¯„
            if EVALUATION_SWITCHES["diversity"]:
                print(f"\nğŸ” æ‰§è¡Œå¤šæ ·æ€§æµ‹è¯„ (æ¨¡å‹: {current_model})...")
                diversity_results = calculate_diversity_metrics(file2_data, file3_data)
                evaluation_results["diversity"] = diversity_results
            else:
                print(f"\nâ­ï¸ è·³è¿‡å¤šæ ·æ€§æµ‹è¯„ (æ¨¡å‹: {current_model})")
                evaluation_results["diversity"] = {"skipped": True, "reason": "ç”¨æˆ·å…³é—­"}
            
            if EVALUATION_SWITCHES["consistency"]:
                print(f"\nğŸ¯ æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯„ (æ¨¡å‹: {current_model})...")
                consistency_results = calculate_consistency_metrics(file2_data, current_model)
                evaluation_results["consistency"] = consistency_results
            else:
                print(f"\nâ­ï¸ è·³è¿‡ä¸€è‡´æ€§æµ‹è¯„ (æ¨¡å‹: {current_model})")
                evaluation_results["consistency"] = {"skipped": True, "reason": "ç”¨æˆ·å…³é—­"}
            
            if EVALUATION_SWITCHES["winrate"]:
                print(f"\nğŸ† æ‰§è¡ŒWin Rateæµ‹è¯„ (æ¨¡å‹: {current_model})...")
                winrate_results = calculate_winrate_metrics(file1_data, file2_data, current_model)
                evaluation_results["winrate"] = winrate_results
            else:
                print(f"\nâ­ï¸ è·³è¿‡Win Rateæµ‹è¯„ (æ¨¡å‹: {current_model})")
                evaluation_results["winrate"] = {"skipped": True, "reason": "ç”¨æˆ·å…³é—­"}
            
            # ä¿å­˜å½“å‰æ¨¡å‹çš„ç»“æœ
            all_results[current_model] = evaluation_results
            
            # ä¿å­˜å½“å‰æ¨¡å‹çš„æµ‹è¯„ç»“æœ
            output_filename = f"unified_evaluation_{timestamp}_{current_model}.json"
            output_path = output_dir / output_filename
            
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ¨¡å‹ {current_model} çš„æµ‹è¯„ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        # å¦‚æœæœ‰å¤šä¸ªæ¨¡å‹ï¼Œåˆ›å»ºæ±‡æ€»ç»“æœ
        if len(models) > 1:
            summary_results = {
                "timestamp": timestamp,
                "models": models,
                "file1_path": file1_path,
                "file2_path": file2_path,
                "file3_path": file3_path,
                "file1_count": len(file1_data),
                "file2_count": len(file2_data),
                "file3_count": len(file3_data),
                "index_range": INDEX_RANGE if INDEX_RANGE["enabled"] else None,
                "evaluation_switches": EVALUATION_SWITCHES,
                "results_by_model": all_results
            }
            
            # ä¿å­˜æ±‡æ€»ç»“æœ
            summary_filename = f"unified_evaluation_summary_{timestamp}.json"
            summary_path = output_dir / summary_filename
            
            with open(summary_path, "w", encoding="utf-8") as f:
                json.dump(summary_results, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ“Š å¤šæ¨¡å‹æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_path}")
            
            # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
            print(f"\n{'='*60}")
            print("ğŸ“Š å¤šæ¨¡å‹æµ‹è¯„æ±‡æ€»")
            print(f"{'='*60}")
            
            for model in models:
                result = all_results[model]
                print(f"\nğŸ¤– æ¨¡å‹: {model}")
                
                # æ˜¾ç¤ºå„é¡¹æµ‹è¯„ç»“æœ
                if "diversity" in result and "error" not in result["diversity"] and "skipped" not in result["diversity"]:
                    div = result["diversity"]
                    print(f"   ğŸ” å¤šæ ·æ€§: BLEU={div['avg_BLEU']:.4f}, METEOR={div['avg_METEOR']:.4f}")
                elif "diversity" in result and "skipped" in result["diversity"]:
                    print(f"   ğŸ” å¤šæ ·æ€§: â­ï¸ å·²è·³è¿‡")
                
                if "consistency" in result and "error" not in result["consistency"] and "skipped" not in result["consistency"]:
                    cons = result["consistency"]
                    print(f"   ğŸ¯ ä¸€è‡´æ€§: æ€»å¹³å‡åˆ† {cons['statistics']['æ€»å¹³å‡åˆ†']}")
                elif "consistency" in result and "skipped" in result["consistency"]:
                    print(f"   ğŸ¯ ä¸€è‡´æ€§: â­ï¸ å·²è·³è¿‡")
                
                if "winrate" in result and "error" not in result["winrate"] and "skipped" not in result["winrate"]:
                    wr = result["winrate"]
                    print(f"   ğŸ† Win Rate: {wr['win_rate']:.4f} ({wr['wins']}/{wr['total']})")
                elif "winrate" in result and "skipped" in result["winrate"]:
                    print(f"   ğŸ† Win Rate: â­ï¸ å·²è·³è¿‡")
        
        else:
            # å•ä¸ªæ¨¡å‹çš„æƒ…å†µï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
            evaluation_results = all_results[models[0]]
            
            # ä¿å­˜å•ä¸ªæ¨¡å‹çš„ç»“æœ
            output_file = output_dir / f"unified_evaluation_{timestamp}.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
            
            print("\n" + "=" * 60)
            print("ğŸ“Š æµ‹è¯„ç»“æœæ±‡æ€»:")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            
            # æ˜¾ç¤ºæµ‹è¯„å¼€å…³çŠ¶æ€
            print(f"\nğŸ”§ æµ‹è¯„å¼€å…³çŠ¶æ€:")
            for switch, enabled in EVALUATION_SWITCHES.items():
                status = "âœ… å¼€å¯" if enabled else "âŒ å…³é—­"
                print(f"  {switch}: {status}")
            
            # æ˜¾ç¤ºæµ‹è¯„ç»“æœ
            if "diversity" in evaluation_results and "error" not in evaluation_results["diversity"] and "skipped" not in evaluation_results["diversity"]:
                div = evaluation_results["diversity"]
                print(f"\nğŸ” å¤šæ ·æ€§ç»“æœ:")
                print(f"   BLEU: {div['avg_BLEU']:.4f}")
                print(f"   METEOR: {div['avg_METEOR']:.4f}")
                print(f"   ROUGE-L: {div['avg_ROUGE_L']:.4f}")
                print(f"   BERTScore-F1: {div['avg_BERTScore_F1']:.4f}")
                print(f"   æ¯”è¾ƒé¢˜ç›®æ•°: {div['matched_question_count']}")
            elif "diversity" in evaluation_results and "skipped" in evaluation_results["diversity"]:
                print(f"\nğŸ” å¤šæ ·æ€§: â­ï¸ å·²è·³è¿‡")
            
            if "consistency" in evaluation_results and "error" not in evaluation_results["consistency"] and "skipped" not in evaluation_results["consistency"]:
                cons = evaluation_results["consistency"]
                print(f"\nğŸ¯ ä¸€è‡´æ€§ç»“æœ: æ€»å¹³å‡åˆ† {cons['statistics']['æ€»å¹³å‡åˆ†']}")
            elif "consistency" in evaluation_results and "skipped" in evaluation_results["consistency"]:
                print(f"\nğŸ¯ ä¸€è‡´æ€§: â­ï¸ å·²è·³è¿‡")
            
            if "winrate" in evaluation_results and "error" not in evaluation_results["winrate"] and "skipped" not in evaluation_results["winrate"]:
                wr = evaluation_results["winrate"]
                print(f"\nğŸ† Win Rateç»“æœ: {wr['win_rate']:.4f} ({wr['wins']}/{wr['total']})")
            elif "winrate" in evaluation_results and "skipped" in evaluation_results["winrate"]:
                print(f"\nğŸ† Win Rate: â­ï¸ å·²è·³è¿‡")
        
        print("\nâœ… ç»Ÿä¸€æµ‹è¯„å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯„è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
