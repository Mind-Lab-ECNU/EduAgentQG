#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ç”Ÿæˆå’Œæµ‹è¯„ç³»ç»Ÿ
æ”¯æŒå¤šç§ç”Ÿæˆæ–¹æ³•å’Œæµ‹è¯„é€‰é¡¹çš„ç»Ÿä¸€è°ƒç”¨
"""

import asyncio
import os
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import argparse
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# å¯¼å…¥å„ä¸ªç”Ÿæˆæ–¹æ³•
from multi_agent_system_v3 import MultiAgentSystemV3
from baseline.COT import main as cot_main
from baseline.COT_n import main as cot_n_main
from baseline.COT_blank import main as cot_blank_main
from baseline.COT_n_blank import main as cot_n_blank_main
from baseline.ReACT import main as react_main
from baseline.ReACT_blank import main as react_blank_main

# å¯¼å…¥æµ‹è¯„æ¨¡å—
from eval.unified_evaluation import main as eval_choice_main
from eval.unified_evaluation_blank import main as eval_blank_main

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('unified_generation_evaluation.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UnifiedGenerationEvaluation:
    """ç»Ÿä¸€ç”Ÿæˆå’Œæµ‹è¯„ç³»ç»Ÿ"""
    
    def __init__(self):
        self.methods = {
            # V3æ–¹æ³•ï¼ˆæˆ‘ä»¬çš„æ–¹æ³•ï¼‰
            "v3_choice": {
                "name": "V3é€‰æ‹©é¢˜ç”Ÿæˆ",
                "description": "å¤šæ™ºèƒ½ä½“ç³»ç»ŸV3 - é€‰æ‹©é¢˜ç”Ÿæˆ",
                "type": "v3",
                "question_type": "choice"
            },
            "v3_blank": {
                "name": "V3å¡«ç©ºé¢˜ç”Ÿæˆ", 
                "description": "å¤šæ™ºèƒ½ä½“ç³»ç»ŸV3 - å¡«ç©ºé¢˜ç”Ÿæˆ",
                "type": "v3",
                "question_type": "blank"
            },
            
            # åŸºçº¿æ–¹æ³• - é€‰æ‹©é¢˜
            "cot_choice": {
                "name": "COTé€‰æ‹©é¢˜",
                "description": "Chain of Thought - é€‰æ‹©é¢˜ç”Ÿæˆ",
                "type": "baseline",
                "question_type": "choice"
            },
            "cot_n_choice": {
                "name": "COT-Né€‰æ‹©é¢˜",
                "description": "Chain of Thought Best-of-N - é€‰æ‹©é¢˜ç”Ÿæˆ", 
                "type": "baseline",
                "question_type": "choice"
            },
            "react_choice": {
                "name": "ReACTé€‰æ‹©é¢˜",
                "description": "ReACT - é€‰æ‹©é¢˜ç”Ÿæˆ",
                "type": "baseline", 
                "question_type": "choice"
            },
            
            # åŸºçº¿æ–¹æ³• - å¡«ç©ºé¢˜
            "cot_blank": {
                "name": "COTå¡«ç©ºé¢˜",
                "description": "Chain of Thought - å¡«ç©ºé¢˜ç”Ÿæˆ",
                "type": "baseline",
                "question_type": "blank"
            },
            "cot_n_blank": {
                "name": "COT-Nå¡«ç©ºé¢˜", 
                "description": "Chain of Thought Best-of-N - å¡«ç©ºé¢˜ç”Ÿæˆ",
                "type": "baseline",
                "question_type": "blank"
            },
            "react_blank": {
                "name": "ReACTå¡«ç©ºé¢˜",
                "description": "ReACT - å¡«ç©ºé¢˜ç”Ÿæˆ", 
                "type": "baseline",
                "question_type": "blank"
            }
        }
        
        self.data_files = {
            "choice": {
                "standard": r"D:\CODE\three_0921\data\choice_unquie_500.jsonl",
                "description": "é€‰æ‹©é¢˜é‡‘æ ‡å‡†æ•°æ®"
            },
            "blank": {
                "standard": r"D:\CODE\three_0921\data\blank_unique.jsonl", 
                "description": "å¡«ç©ºé¢˜é‡‘æ ‡å‡†æ•°æ®"
            }
        }
        
        self.output_dir = Path(r"D:\CODE\three_0921\unified_outputs")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def list_methods(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç”Ÿæˆæ–¹æ³•"""
        print("=" * 80)
        print("ğŸ“‹ å¯ç”¨çš„ç”Ÿæˆæ–¹æ³•:")
        print("=" * 80)
        
        for method_id, method_info in self.methods.items():
            print(f"ğŸ”¹ {method_id}")
            print(f"   åç§°: {method_info['name']}")
            print(f"   æè¿°: {method_info['description']}")
            print(f"   ç±»å‹: {method_info['type']}")
            print(f"   é¢˜å‹: {method_info['question_type']}")
            print()
    
    def list_data_files(self):
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®æ–‡ä»¶"""
        print("=" * 80)
        print("ğŸ“ å¯ç”¨çš„æ•°æ®æ–‡ä»¶:")
        print("=" * 80)
        
        for data_type, file_info in self.data_files.items():
            print(f"ğŸ”¹ {data_type}")
            print(f"   è·¯å¾„: {file_info['standard']}")
            print(f"   æè¿°: {file_info['description']}")
            print(f"   å­˜åœ¨: {'âœ…' if os.path.exists(file_info['standard']) else 'âŒ'}")
            print()
    
    def validate_inputs(self, method_id: str, data_file: str = None, 
                       start_index: int = 0, end_index: int = None) -> Tuple[bool, str]:
        """éªŒè¯è¾“å…¥å‚æ•°"""
        # éªŒè¯æ–¹æ³•ID
        if method_id not in self.methods:
            return False, f"æœªçŸ¥çš„æ–¹æ³•ID: {method_id}"
        
        # éªŒè¯æ•°æ®æ–‡ä»¶
        if data_file and not os.path.exists(data_file):
            return False, f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}"
        
        # éªŒè¯ç´¢å¼•èŒƒå›´
        if start_index < 0:
            return False, "å¼€å§‹ç´¢å¼•ä¸èƒ½ä¸ºè´Ÿæ•°"
        
        if end_index is not None and end_index <= start_index:
            return False, "ç»“æŸç´¢å¼•å¿…é¡»å¤§äºå¼€å§‹ç´¢å¼•"
        
        return True, "å‚æ•°éªŒè¯é€šè¿‡"
    
    async def run_with_params(self, method_id: str, start_index: int = 0, end_index: int = 10,
                             data_file: str = None, batch_size: int = 1, 
                             delay_between_batches: float = 2.0, use_rag: bool = True,
                             rag_mode: str = "planner", evaluation_mode: str = "binary",
                             enable_consistency: bool = True, enable_winrate: bool = True,
                             enable_diversity: bool = False, run_twice_for_consistency: bool = True,
                             evaluation_model = "gpt-4o", evaluation_models = None, 
                             base_model_name = "gemini") -> Dict[str, Any]:
        """é€šè¿‡ä»£ç å‚æ•°ç›´æ¥è¿è¡Œå®Œæ•´æµç¨‹"""
        logger.info("ğŸš€ é€šè¿‡ä»£ç å‚æ•°è¿è¡Œå®Œæ•´æµç¨‹")
        logger.info(f"ğŸ“‹ æ–¹æ³•: {method_id}, èŒƒå›´: {start_index}-{end_index}")
        
        # å¤„ç†æµ‹è¯„æ¨¡å‹å‚æ•°
        if evaluation_models is not None:
            # ä½¿ç”¨å¤šä¸ªæµ‹è¯„æ¨¡å‹
            return await self.run_complete_pipeline(
                method_id=method_id,
                data_file=data_file,
                start_index=start_index,
                end_index=end_index,
                batch_size=batch_size,
                delay_between_batches=delay_between_batches,
                use_rag=use_rag,
                rag_mode=rag_mode,
                evaluation_mode=evaluation_mode,
                enable_consistency=enable_consistency,
                enable_winrate=enable_winrate,
                enable_diversity=enable_diversity,
                run_twice_for_consistency=run_twice_for_consistency,
                evaluation_models=evaluation_models,
                base_model_name=base_model_name
            )
        else:
            # ä½¿ç”¨å•ä¸ªæµ‹è¯„æ¨¡å‹
            return await self.run_complete_pipeline(
                method_id=method_id,
                data_file=data_file,
                start_index=start_index,
                end_index=end_index,
                batch_size=batch_size,
                delay_between_batches=delay_between_batches,
                use_rag=use_rag,
                rag_mode=rag_mode,
                evaluation_mode=evaluation_mode,
                enable_consistency=enable_consistency,
                enable_winrate=enable_winrate,
                enable_diversity=enable_diversity,
                run_twice_for_consistency=run_twice_for_consistency,
                evaluation_model=evaluation_model,
                base_model_name=base_model_name
        )
    
    async def generate_questions(self, method_id: str, data_file: str = None,
                               start_index: int = 0, end_index: int = None,
                               batch_size: int = 5, delay_between_batches: float = 1.0,
                               use_rag: bool = True, rag_mode: str = "planner",
                               evaluation_mode: str = "binary", base_model_name: str = "gemini") -> Dict[str, Any]:
        """ç”Ÿæˆé¢˜ç›®"""
        method_info = self.methods[method_id]
        question_type = method_info["question_type"]
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤çš„
        if not data_file:
            data_file = self.data_files[question_type]["standard"]
        
        logger.info(f"ğŸš€ å¼€å§‹ä½¿ç”¨ {method_info['name']} ç”Ÿæˆé¢˜ç›®")
        logger.info(f"ğŸ“ æ•°æ®æ–‡ä»¶: {data_file}")
        logger.info(f"ğŸ“Š ç´¢å¼•èŒƒå›´: {start_index} - {end_index}")
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            if method_info["type"] == "v3":
                # V3æ–¹æ³•
                return await self._generate_v3_questions(
                    method_id, data_file, start_index, end_index,
                    batch_size, delay_between_batches, use_rag, rag_mode, evaluation_mode, base_model_name
                )
            else:
                # åŸºçº¿æ–¹æ³•
                return await self._generate_baseline_questions(
                    method_id, data_file, start_index, end_index, timestamp, base_model_name, 1
                )
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆé¢˜ç›®å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "output_file": None,
                "method": method_info["name"]
            }
    
    async def _generate_v3_questions(self, method_id: str, data_file: str,
                                   start_index: int, end_index: int,
                                   batch_size: int, delay_between_batches: float,
                                   use_rag: bool, rag_mode: str, evaluation_mode: str, base_model_name: str) -> Dict[str, Any]:
        """ä½¿ç”¨V3æ–¹æ³•ç”Ÿæˆé¢˜ç›®"""
        from datetime import datetime
        start_time = datetime.now()
        
        try:
            # æ ¹æ®base_model_nameè®¾ç½®æ¨¡å‹é…ç½®
            if base_model_name == "qwen-72b":
                # è®¾ç½®Qwen-72Bé…ç½® - æ›´æ–°æ‰€æœ‰5ä¸ªæ™ºèƒ½ä½“çš„é…ç½®
                from config import update_model_configs_for_qwen
                update_model_configs_for_qwen()
                logger.info("ğŸ¤– æ‰€æœ‰5ä¸ªæ™ºèƒ½ä½“å·²åˆ‡æ¢åˆ°Qwen-72Bæ¨¡å‹")
            elif base_model_name == "gemini":
                # ä½¿ç”¨é»˜è®¤Geminié…ç½®
                from config import update_model_configs_for_gemini
                update_model_configs_for_gemini()
                logger.info("ğŸ¤– æ‰€æœ‰5ä¸ªæ™ºèƒ½ä½“å·²åˆ‡æ¢åˆ°Geminiæ¨¡å‹")
            elif base_model_name == "gpt-4o-mini":
                # ä½¿ç”¨GPT-4o-minié…ç½®
                from config import update_model_configs_for_gpt4o_mini
                update_model_configs_for_gpt4o_mini()
                logger.info("ğŸ¤– æ‰€æœ‰5ä¸ªæ™ºèƒ½ä½“å·²åˆ‡æ¢åˆ°GPT-4o-miniæ¨¡å‹")
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„base_model_name: {base_model_name}ï¼Œä½¿ç”¨é»˜è®¤Geminié…ç½®")
            
            # åˆ›å»ºV3ç³»ç»Ÿ
            system = MultiAgentSystemV3(
                use_rag=use_rag,
                rag_mode=rag_mode, 
                evaluation_mode=evaluation_mode
            )
            
            # åˆå§‹åŒ–ç³»ç»Ÿ
            await system.initialize()
            
            # ç”Ÿæˆé¢˜ç›®
            result = await system.generate_questions_from_data(
                jsonl_file_path=data_file,
                batch_size=batch_size,
                delay_between_batches=delay_between_batches,
                start_index=start_index,
                end_index=end_index
            )
            
            # å…³é—­ç³»ç»Ÿ
            await system.shutdown()
            
            return {
                "success": True,
                "output_file": result["generated"],
                "workflow_file": result["workflow"],
                "method": self.methods[method_id]["name"],
                "start_time": start_time
            }
            
        except Exception as e:
            logger.error(f"V3ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def _generate_baseline_questions(self, method_id: str, data_file: str,
                                         start_index: int, end_index: int,
                                         timestamp: str, base_model_name: str, 
                                         generation_attempt: int = 1) -> Dict[str, Any]:
        """ä½¿ç”¨åŸºçº¿æ–¹æ³•ç”Ÿæˆé¢˜ç›®"""
        from datetime import datetime
        start_time = datetime.now()
        
        try:
            # æ ¹æ®base_model_nameè®¾ç½®æ¨¡å‹é…ç½®
            if base_model_name == "qwen-72b":
                # è®¾ç½®Qwen-72Bé…ç½®
                import os
                os.environ["OPENAI_BASE_URL"] = "https://notebook-inspire.sii.edu.cn/ws-9dcc0e1f-80a4-4af2-bc2f-0e352e7b17e6/project-b795c114-135a-40db-b3d0-19b60f25237b/user-304c6bd0-a3e9-4e9d-826c-dace2a1d04bd/vscode/62fd4373-8a6a-40fd-86c8-4077fa381f74/49942f40-349f-4291-b3a0-d6886c8d2da5/proxy/33001/v1"
                os.environ["OPENAI_API_KEY"] = "sk-pjtlgoubuigtgpxneosvmivvvkopxflxfncnhorzenbasdyb"
                os.environ["OPENAI_MODEL"] = "Qwen/Qwen2.5-72B-Instruct"
                logger.info("ğŸ¤– åŸºçº¿æ–¹æ³•ä½¿ç”¨Qwen-72Bæ¨¡å‹é…ç½®")
            elif base_model_name == "gemini":
                # ä½¿ç”¨é»˜è®¤Geminié…ç½®
                import os
                os.environ["OPENAI_MODEL"] = "gemini-2.5-flash"
                logger.info("ğŸ¤– åŸºçº¿æ–¹æ³•ä½¿ç”¨Geminiæ¨¡å‹é…ç½®")
            elif base_model_name == "gpt-4o-mini":
                # ä½¿ç”¨GPT-4o-minié…ç½®
                import os
                os.environ["OPENAI_MODEL"] = "gpt-4o-mini"
                logger.info("ğŸ¤– åŸºçº¿æ–¹æ³•ä½¿ç”¨GPT-4o-miniæ¨¡å‹é…ç½®")
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„base_model_name: {base_model_name}ï¼ŒåŸºçº¿æ–¹æ³•ä½¿ç”¨é»˜è®¤Geminié…ç½®")
            
            # æ ¹æ®æ–¹æ³•IDé€‰æ‹©å¯¹åº”çš„å‡½æ•°
            method_functions = {
                "cot_choice": cot_main,
                "cot_n_choice": cot_n_main,
                "cot_blank": cot_blank_main,
                "cot_n_blank": cot_n_blank_main,
                "react_choice": react_main,
                "react_blank": react_blank_main
            }
            
            if method_id not in method_functions:
                raise ValueError(f"ä¸æ”¯æŒçš„åŸºçº¿æ–¹æ³•: {method_id}")
            
            # ä¸ºç¬¬äºŒæ¬¡ç”Ÿæˆæ·»åŠ ä¸åŒçš„æ—¶é—´æˆ³åç¼€
            if generation_attempt > 1:
                timestamp = f"{timestamp}_attempt_{generation_attempt}"
            
            # è°ƒç”¨å¯¹åº”çš„åŸºçº¿æ–¹æ³•
            output_file = method_functions[method_id](
                data_file=data_file,
                start_idx=start_index,
                end_idx=end_index,
                timestamp=timestamp
            )
            
            return {
                "success": True,
                "output_file": output_file,
                "workflow_file": None,
                "method": self.methods[method_id]["name"],
                "start_time": start_time
            }
            
        except Exception as e:
            logger.error(f"åŸºçº¿æ–¹æ³•ç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def evaluate_questions(self, method_id: str, generated_file: str,
                          second_generated_file: str = None,
                          start_index: int = 0, end_index: int = None,
                          enable_consistency: bool = True, enable_winrate: bool = True,
                          enable_diversity: bool = False, evaluation_model = "gpt-4o") -> Dict[str, Any]:
        """æµ‹è¯„ç”Ÿæˆçš„é¢˜ç›®"""
        method_info = self.methods[method_id]
        question_type = method_info["question_type"]
        
        logger.info(f"ğŸ“Š å¼€å§‹æµ‹è¯„ {method_info['name']} ç”Ÿæˆçš„é¢˜ç›®")
        logger.info(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {generated_file}")
        
        try:
            if question_type == "choice":
                # é€‰æ‹©é¢˜æµ‹è¯„
                return self._evaluate_choice_questions(
                    generated_file, second_generated_file, start_index, end_index,
                    enable_consistency, enable_winrate, enable_diversity, evaluation_model
                )
            else:
                # å¡«ç©ºé¢˜æµ‹è¯„
                return self._evaluate_blank_questions(
                    generated_file, second_generated_file, start_index, end_index,
                    enable_consistency, enable_winrate, enable_diversity, evaluation_model
                )
                
        except Exception as e:
            logger.error(f"æµ‹è¯„å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "evaluation_file": None
            }
    
    def _evaluate_choice_questions(self, generated_file: str, second_generated_file: str,
                                 start_index: int, end_index: int, enable_consistency: bool,
                                 enable_winrate: bool, enable_diversity: bool, evaluation_model: str = "gpt-4o") -> Dict[str, Any]:
        """æµ‹è¯„é€‰æ‹©é¢˜"""
        try:
            # åŠ¨æ€è®¾ç½®æµ‹è¯„æ¨¡å—çš„æ–‡ä»¶è·¯å¾„
            import eval.unified_evaluation as eval_module
            
            # è®¾ç½®æ–‡ä»¶è·¯å¾„
            eval_module.file1_path = self.data_files["choice"]["standard"]  # é‡‘æ ‡å‡†æ•°æ®ï¼ˆç”¨äºWin Rateæµ‹è¯„ï¼‰
            eval_module.file2_path = generated_file  # ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆç”¨äºä¸€è‡´æ€§æµ‹è¯„å’ŒWin Rateæµ‹è¯„ï¼‰
            eval_module.file3_path = second_generated_file if second_generated_file else generated_file  # ç¬¬äºŒæ¬¡ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆç”¨äºå¤šæ ·æ€§æµ‹è¯„ï¼‰
            
            # è®¾ç½®ç´¢å¼•èŒƒå›´ - å¯¹äºç”Ÿæˆçš„æ–‡ä»¶ï¼Œç´¢å¼•åº”è¯¥ä»0å¼€å§‹
            eval_module.INDEX_RANGE = {
                "enabled": False,  # ç¦ç”¨ç´¢å¼•è¿‡æ»¤ï¼Œå› ä¸ºç”Ÿæˆçš„æ–‡ä»¶å·²ç»æ˜¯å¯¹åº”èŒƒå›´çš„æ•°æ®
                "start_index": 0,
                "end_index": end_index - start_index if end_index else None
            }
            
            # è®¾ç½®æµ‹è¯„é€‰é¡¹
            eval_module.EVALUATION_SWITCHES = {
                "diversity": enable_diversity,
                "consistency": enable_consistency,
                "winrate": enable_winrate
            }
            
            # è°ƒç”¨é€‰æ‹©é¢˜æµ‹è¯„ï¼Œä¼ å…¥æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
            eval_choice_main(
                file1=self.data_files["choice"]["standard"],  # é‡‘æ ‡å‡†æ•°æ®
                file2=generated_file,  # ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„æ•°æ®
                file3=second_generated_file,  # ç¬¬äºŒæ¬¡ç”Ÿæˆçš„æ•°æ®ï¼ˆå¤šæ ·æ€§æ¯”è¾ƒç”¨ï¼‰
                eval_model=evaluation_model,
                start_idx=0,  # ç”Ÿæˆçš„æ–‡ä»¶ç´¢å¼•ä»0å¼€å§‹
                end_idx=end_index - start_index if end_index else None  # è°ƒæ•´ç»“æŸç´¢å¼•
            )
            return {
                "success": True,
                "evaluation_file": None,  # æµ‹è¯„æ¨¡å—ä¼šç›´æ¥ä¿å­˜æ–‡ä»¶ï¼Œæˆ‘ä»¬ç¨åè·å–è·¯å¾„
                "method": "é€‰æ‹©é¢˜æµ‹è¯„"
            }
        except Exception as e:
            logger.error(f"é€‰æ‹©é¢˜æµ‹è¯„å¤±è´¥: {e}")
            raise
    
    def _evaluate_blank_questions(self, generated_file: str, second_generated_file: str,
                                start_index: int, end_index: int, enable_consistency: bool,
                                enable_winrate: bool, enable_diversity: bool, evaluation_model: str = "gpt-4o") -> Dict[str, Any]:
        """æµ‹è¯„å¡«ç©ºé¢˜"""
        try:
            # åŠ¨æ€è®¾ç½®æµ‹è¯„æ¨¡å—çš„æ–‡ä»¶è·¯å¾„
            import eval.unified_evaluation_blank as eval_module
            
            # è®¾ç½®æ–‡ä»¶è·¯å¾„
            eval_module.file1_path = self.data_files["blank"]["standard"]  # é‡‘æ ‡å‡†æ•°æ®ï¼ˆç”¨äºWin Rateæµ‹è¯„ï¼‰
            eval_module.file2_path = generated_file  # ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆç”¨äºä¸€è‡´æ€§æµ‹è¯„å’ŒWin Rateæµ‹è¯„ï¼‰
            eval_module.file3_path = second_generated_file if second_generated_file else generated_file  # ç¬¬äºŒæ¬¡ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆç”¨äºå¤šæ ·æ€§æµ‹è¯„ï¼‰
            
            # è®¾ç½®ç´¢å¼•èŒƒå›´ - å¯¹äºç”Ÿæˆçš„æ–‡ä»¶ï¼Œç´¢å¼•åº”è¯¥ä»0å¼€å§‹
            eval_module.INDEX_RANGE = {
                "enabled": False,  # ç¦ç”¨ç´¢å¼•è¿‡æ»¤ï¼Œå› ä¸ºç”Ÿæˆçš„æ–‡ä»¶å·²ç»æ˜¯å¯¹åº”èŒƒå›´çš„æ•°æ®
                "start_index": 0,
                "end_index": end_index - start_index if end_index else None
            }
            
            # è®¾ç½®æµ‹è¯„é€‰é¡¹
            eval_module.EVALUATION_SWITCHES = {
                "diversity": enable_diversity,
                "consistency": enable_consistency,
                "winrate": enable_winrate
            }
            
            # è°ƒç”¨å¡«ç©ºé¢˜æµ‹è¯„ï¼Œä¼ å…¥æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„
            eval_blank_main(
                file1=self.data_files["blank"]["standard"],  # é‡‘æ ‡å‡†æ•°æ®
                file2=generated_file,  # ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„æ•°æ®
                file3=second_generated_file,  # ç¬¬äºŒæ¬¡ç”Ÿæˆçš„æ•°æ®ï¼ˆå¤šæ ·æ€§æ¯”è¾ƒç”¨ï¼‰
                eval_model=evaluation_model,
                start_idx=0,  # ç”Ÿæˆçš„æ–‡ä»¶ç´¢å¼•ä»0å¼€å§‹
                end_idx=end_index - start_index if end_index else None  # è°ƒæ•´ç»“æŸç´¢å¼•
            )
            return {
                "success": True,
                "evaluation_file": None,  # æµ‹è¯„æ¨¡å—ä¼šç›´æ¥ä¿å­˜æ–‡ä»¶ï¼Œæˆ‘ä»¬ç¨åè·å–è·¯å¾„
                "method": "å¡«ç©ºé¢˜æµ‹è¯„"
            }
        except Exception as e:
            logger.error(f"å¡«ç©ºé¢˜æµ‹è¯„å¤±è´¥: {e}")
            raise
    
    def _normalize_evaluation_models(self, evaluation_model):
        """æ ‡å‡†åŒ–æµ‹è¯„æ¨¡å‹å‚æ•°ï¼Œæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªæ¨¡å‹"""
        if isinstance(evaluation_model, str):
            return [evaluation_model]
        elif isinstance(evaluation_model, (list, tuple)):
            return list(evaluation_model)
        else:
            return ["gpt-4o"]  # é»˜è®¤å€¼
    
    def _generate_evaluation_file_path(self, base_file: str, model_name: str, timestamp: str, method_name: str = None) -> str:
        """ä¸ºä¸åŒæµ‹è¯„æ¨¡å‹ç”Ÿæˆç‹¬ç«‹çš„æµ‹è¯„æ–‡ä»¶è·¯å¾„"""
        import os
        
        # è·å–åŸºç¡€æ–‡ä»¶åå’Œç›®å½•
        base_dir = os.path.dirname(base_file)
        base_name = os.path.basename(base_file)
        name_without_ext = os.path.splitext(base_name)[0]
        ext = os.path.splitext(base_name)[1]
        
        # æ¸…ç†æ¨¡å‹åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_model_name = model_name.replace(":", "_").replace("/", "_").replace("-", "_")
        
        # æ¸…ç†æ–¹æ³•åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        clean_method_name = ""
        if method_name:
            clean_method_name = method_name.replace(":", "_").replace("/", "_").replace("-", "_").replace(" ", "_")
            clean_method_name = f"{clean_method_name}_"
        
        # ç”Ÿæˆæ–°çš„æ–‡ä»¶å
        new_filename = f"{name_without_ext}_eval_{clean_method_name}{clean_model_name}_{timestamp}{ext}"
        return os.path.join(base_dir, new_filename)
    
    async def run_complete_pipeline(self, method_id: str, data_file: str = None,
                                  start_index: int = 0, end_index: int = None,
                                  batch_size: int = 1, delay_between_batches: float = 2.0,
                                  use_rag: bool = True, rag_mode: str = "planner",
                                  evaluation_mode: str = "binary",
                                  enable_consistency: bool = True, enable_winrate: bool = True,
                                  enable_diversity: bool = False, run_twice_for_consistency: bool = True,
                                  evaluation_model = "gpt-4o", evaluation_models = None, 
                                  base_model_name = "gemini") -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ç”Ÿæˆ+æµ‹è¯„æµç¨‹"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æµç¨‹ï¼šç”Ÿæˆ + æµ‹è¯„")
        logger.info("=" * 80)
        
        # éªŒè¯å‚æ•°
        is_valid, error_msg = self.validate_inputs(method_id, data_file, start_index, end_index)
        if not is_valid:
            return {"success": False, "error": error_msg}
        
        # ç”Ÿæˆé¢˜ç›®
        logger.info("ğŸ“ æ­¥éª¤1: ç”Ÿæˆé¢˜ç›®")
        generation_result = await self.generate_questions(
            method_id=method_id,
            data_file=data_file,
            start_index=start_index,
            end_index=end_index,
            batch_size=batch_size,
            delay_between_batches=delay_between_batches,
            use_rag=use_rag,
            rag_mode=rag_mode,
            evaluation_mode=evaluation_mode,
            base_model_name=base_model_name
        )
        
        if not generation_result["success"]:
            return generation_result
        
        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å‘½åï¼ˆéœ€è¦åœ¨ç¬¬äºŒæ¬¡ç”Ÿæˆä¹‹å‰å®šä¹‰ï¼‰
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¦‚æœéœ€è¦å¤šæ ·æ€§æµ‹è¯„ï¼Œè¿›è¡Œç¬¬äºŒæ¬¡ç”Ÿæˆ
        second_generation_result = None
        if enable_diversity and run_twice_for_consistency:
            logger.info("ğŸ“ æ­¥éª¤1.5: ç¬¬äºŒæ¬¡ç”Ÿæˆï¼ˆç”¨äºå¤šæ ·æ€§æµ‹è¯„ï¼‰")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºV3æ–¹æ³•
            if method_id in ["v3_choice", "v3_blank"]:
                # V3æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨generate_questions
                second_generation_result = await self.generate_questions(
                    method_id=method_id,
                    data_file=data_file,
                    start_index=start_index,
                    end_index=end_index,
                    batch_size=batch_size,
                    delay_between_batches=delay_between_batches,
                    use_rag=use_rag,
                    rag_mode=rag_mode,
                    evaluation_mode=evaluation_mode,
                    base_model_name=base_model_name
                )
            else:
                # åŸºçº¿æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨_generate_baseline_questionsï¼Œä¼ å…¥generation_attempt=2
                second_generation_result = await self._generate_baseline_questions(
                    method_id=method_id,
                    data_file=data_file,
                    start_index=start_index,
                    end_index=end_index,
                    timestamp=timestamp,
                    base_model_name=base_model_name,
                    generation_attempt=2
                )
            
            if not second_generation_result["success"]:
                logger.warning("âš ï¸ ç¬¬äºŒæ¬¡ç”Ÿæˆå¤±è´¥ï¼Œå°†åªä½¿ç”¨ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„ç»“æœè¿›è¡Œæµ‹è¯„")
                second_generation_result = None
        
        # æµ‹è¯„é¢˜ç›® - æ”¯æŒå¤šæ¨¡å‹æµ‹è¯„
        logger.info("ğŸ“Š æ­¥éª¤2: æµ‹è¯„é¢˜ç›®")
        
        # æ ‡å‡†åŒ–æµ‹è¯„æ¨¡å‹åˆ—è¡¨
        if evaluation_models is not None:
            # ä½¿ç”¨æä¾›çš„å¤šä¸ªæµ‹è¯„æ¨¡å‹
            evaluation_models = self._normalize_evaluation_models(evaluation_models)
        else:
            # ä½¿ç”¨å•ä¸ªæµ‹è¯„æ¨¡å‹
            evaluation_models = self._normalize_evaluation_models(evaluation_model)
        logger.info(f"ğŸ¤– å°†ä½¿ç”¨ {len(evaluation_models)} ä¸ªæµ‹è¯„æ¨¡å‹: {evaluation_models}")
        
        # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„æµ‹è¯„ç»“æœ
        evaluation_results = {}
        
        for i, model in enumerate(evaluation_models, 1):
            logger.info(f"ğŸ“Š æ­¥éª¤2.{i}: ä½¿ç”¨æµ‹è¯„æ¨¡å‹ {model}")
            
            try:
                # ä¸ºæ¯ä¸ªæ¨¡å‹ç”Ÿæˆç‹¬ç«‹çš„æµ‹è¯„æ–‡ä»¶è·¯å¾„
                base_generated_file = generation_result["output_file"]
                base_second_file = second_generation_result["output_file"] if second_generation_result else None
                
                # è·å–æ–¹æ³•åç§°
                method_name = self.methods[method_id]["name"]
                
                # ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„è¾“å…¥æ–‡ä»¶è·¯å¾„
                model_generated_file = self._generate_evaluation_file_path(
                    base_generated_file, f"{model}_input_1", timestamp, method_name
                )
                model_second_file = self._generate_evaluation_file_path(
                    base_second_file, f"{model}_input_2", timestamp, method_name
                ) if base_second_file else None
                
                # å¤åˆ¶è¾“å…¥æ–‡ä»¶åˆ°æ–°è·¯å¾„ï¼ˆé¿å…ä¿®æ”¹åŸæ–‡ä»¶ï¼‰
                import shutil
                shutil.copy2(base_generated_file, model_generated_file)
                if model_second_file:
                    shutil.copy2(base_second_file, model_second_file)
                
                # ä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæµ‹è¯„
                model_evaluation_result = self.evaluate_questions(
                    method_id=method_id,
                    generated_file=model_generated_file,
                    second_generated_file=model_second_file,
                    start_index=start_index,
                    end_index=end_index,
                    enable_consistency=enable_consistency,
                    enable_winrate=enable_winrate,
                    enable_diversity=enable_diversity,
                    evaluation_model=model
                )
                
                # æµ‹è¯„å®Œæˆåï¼Œè‡ªåŠ¨è·å–ç”Ÿæˆçš„æµ‹è¯„æ–‡ä»¶è·¯å¾„
                if model_evaluation_result["success"]:
                    # æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„æµ‹è¯„æ–‡ä»¶
                    eval_dir = Path("eval/unified_results")
                    if eval_dir.exists():
                        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
                        eval_files = list(eval_dir.glob("unified_evaluation_*.json"))
                        if eval_files:
                            latest_eval_file = max(eval_files, key=lambda x: x.stat().st_mtime)
                            
                            # ä¸ºæµ‹è¯„ç»“æœæ–‡ä»¶ç”Ÿæˆå¸¦æ¨¡å‹åç§°çš„è·¯å¾„
                            model_eval_file = self._generate_evaluation_file_path(
                                str(latest_eval_file), model, timestamp, method_name
                            )
                            
                            # å¤åˆ¶æµ‹è¯„ç»“æœæ–‡ä»¶åˆ°æ–°è·¯å¾„
                            shutil.copy2(str(latest_eval_file), model_eval_file)
                            model_evaluation_result["evaluation_file"] = model_eval_file
                            
                            logger.info(f"ğŸ“ æ‰¾åˆ°æµ‹è¯„æ–‡ä»¶: {latest_eval_file} -> {model_eval_file}")
                        else:
                            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æµ‹è¯„æ–‡ä»¶ï¼Œæ¨¡å‹: {model}")
                            model_evaluation_result["success"] = False
                            model_evaluation_result["error"] = "æœªæ‰¾åˆ°æµ‹è¯„æ–‡ä»¶"
                    else:
                        logger.warning(f"âš ï¸ æµ‹è¯„ç›®å½•ä¸å­˜åœ¨: {eval_dir}")
                        model_evaluation_result["success"] = False
                        model_evaluation_result["error"] = "æµ‹è¯„ç›®å½•ä¸å­˜åœ¨"
                
                # å­˜å‚¨ç»“æœ
                evaluation_results[model] = {
                    "success": model_evaluation_result["success"],
                    "evaluation_file": model_evaluation_result.get("evaluation_file"),
                    "method": f"æµ‹è¯„æ¨¡å‹: {model}",
                    "error": model_evaluation_result.get("error")
                }
                
                if model_evaluation_result["success"]:
                    logger.info(f"âœ… æµ‹è¯„æ¨¡å‹ {model} å®Œæˆ: {model_evaluation_result.get('evaluation_file')}")
                else:
                    logger.error(f"âŒ æµ‹è¯„æ¨¡å‹ {model} å¤±è´¥: {model_evaluation_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯„æ¨¡å‹ {model} å¼‚å¸¸: {e}")
                evaluation_results[model] = {
                    "success": False,
                    "error": str(e),
                    "method": f"æµ‹è¯„æ¨¡å‹: {model}"
                }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„æµ‹è¯„
        successful_evaluations = [model for model, result in evaluation_results.items() if result["success"]]
        if not successful_evaluations:
            return {
                "success": False,
                "error": "æ‰€æœ‰æµ‹è¯„æ¨¡å‹çš„æµ‹è¯„éƒ½å¤±è´¥äº†",
                "evaluation_results": evaluation_results
            }
        
        # è¿”å›å®Œæ•´ç»“æœ
        result = {
            "success": True,
            "generation": generation_result,
            "evaluation_results": evaluation_results,  # å¤šæ¨¡å‹æµ‹è¯„ç»“æœ
            "summary": {
                "method": self.methods[method_id]["name"],
                "data_file": data_file or self.data_files[self.methods[method_id]["question_type"]]["standard"],
                "index_range": f"{start_index}-{end_index}",
                "generated_file": generation_result["output_file"],
                "evaluation_models": evaluation_models,
                "successful_evaluations": successful_evaluations,
                "evaluation_files": {
                    model: result["evaluation_file"] 
                    for model, result in evaluation_results.items() 
                    if result["success"]
                }
            }
        }
        
        # å¦‚æœæœ‰ç¬¬äºŒæ¬¡ç”Ÿæˆï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if second_generation_result:
            result["second_generation"] = second_generation_result
            result["summary"]["second_generated_file"] = second_generation_result["output_file"]
        
        # ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶
        try:
            stats_file = self._generate_statistics_file(
                method_id=method_id,
                base_model_name=base_model_name,
                generation_result=generation_result,
                second_generation_result=second_generation_result,
                evaluation_results=evaluation_results,
                start_index=start_index,
                end_index=end_index,
                timestamp=timestamp
            )
            result["statistics_file"] = stats_file
            logger.info(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶å·²ä¿å­˜: {stats_file}")
        except Exception as e:
            logger.warning(f"âš ï¸ ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶å¤±è´¥: {e}")
        
        # è¾“å‡ºæµ‹è¯„ç»“æœæ‘˜è¦
        logger.info("=" * 80)
        logger.info("ğŸ“Š æµ‹è¯„ç»“æœæ‘˜è¦")
        logger.info("=" * 80)
        for model, eval_result in evaluation_results.items():
            if eval_result["success"]:
                logger.info(f"âœ… {model}: {eval_result['evaluation_file']}")
            else:
                logger.error(f"âŒ {model}: {eval_result.get('error', 'æµ‹è¯„å¤±è´¥')}")
        logger.info("=" * 80)
        
        return result

    def _generate_statistics_file(self, method_id: str, base_model_name: str, 
                                generation_result: Dict[str, Any], 
                                second_generation_result: Dict[str, Any],
                                evaluation_results: Dict[str, Any],
                                start_index: int, end_index: int, 
                                timestamp: str) -> str:
        """ç”Ÿæˆç»Ÿè®¡æ–‡ä»¶ï¼ŒåŒ…å«tokenä½¿ç”¨é‡ã€ç”Ÿæˆé¢˜ç›®æ•°é‡ã€èŠ±è´¹æ—¶é—´ç­‰ä¿¡æ¯"""
        from datetime import datetime
        import json
        import os
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        start_time = generation_result.get("start_time")
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds() if start_time else 0
        
        # è®¡ç®—ç”Ÿæˆé¢˜ç›®æ•°é‡
        generated_count = 0
        if generation_result.get("success") and generation_result.get("output_file"):
            try:
                with open(generation_result["output_file"], "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if isinstance(data, dict) and "results" in data:
                        generated_count = len(data["results"])
                    elif isinstance(data, list):
                        generated_count = len(data)
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è¯»å–ç”Ÿæˆæ–‡ä»¶ç»Ÿè®¡é¢˜ç›®æ•°é‡: {e}")
        
        # è®¡ç®—ç¬¬äºŒæ¬¡ç”Ÿæˆé¢˜ç›®æ•°é‡
        second_generated_count = 0
        if second_generation_result and second_generation_result.get("success") and second_generation_result.get("output_file"):
            try:
                with open(second_generation_result["output_file"], "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if isinstance(data, dict) and "results" in data:
                        second_generated_count = len(data["results"])
                    elif isinstance(data, list):
                        second_generated_count = len(data)
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è¯»å–ç¬¬äºŒæ¬¡ç”Ÿæˆæ–‡ä»¶ç»Ÿè®¡é¢˜ç›®æ•°é‡: {e}")
        
        # è®¡ç®—tokenä½¿ç”¨é‡ï¼ˆä»V3ç³»ç»Ÿè·å–ï¼‰
        token_stats = self._extract_token_statistics(generation_result, second_generation_result)
        
        # è®¡ç®—æµ‹è¯„ç»Ÿè®¡
        evaluation_stats = {
            "total_models": len(evaluation_results),
            "successful_models": len([r for r in evaluation_results.values() if r.get("success", False)]),
            "failed_models": len([r for r in evaluation_results.values() if not r.get("success", False)])
        }
        
        # æ„å»ºç»Ÿè®¡ä¿¡æ¯
        statistics = {
            "run_info": {
                "method_id": method_id,
                "method_name": self.methods[method_id]["name"],
                "base_model_name": base_model_name,
                "timestamp": timestamp,
                "start_time": start_time.isoformat() if start_time else None,
                "end_time": end_time.isoformat(),
                "total_duration_seconds": total_duration,
                "total_duration_formatted": f"{int(total_duration // 3600):02d}:{int((total_duration % 3600) // 60):02d}:{int(total_duration % 60):02d}"
            },
            "generation_stats": {
                "first_generation": {
                    "success": generation_result.get("success", False),
                    "output_file": generation_result.get("output_file"),
                    "questions_generated": generated_count,
                    "start_index": start_index,
                    "end_index": end_index,
                    "range_size": (end_index - start_index + 1) if end_index is not None else "unlimited"
                },
                "second_generation": {
                    "enabled": second_generation_result is not None,
                    "success": second_generation_result.get("success", False) if second_generation_result else False,
                    "output_file": second_generation_result.get("output_file") if second_generation_result else None,
                    "questions_generated": second_generated_count
                },
                "total_questions_generated": generated_count + second_generated_count
            },
            "token_usage": token_stats,
            "evaluation_stats": evaluation_stats,
            "evaluation_files": {
                model: result.get("evaluation_file") 
                for model, result in evaluation_results.items() 
                if result.get("success", False)
            }
        }
        
        # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶
        stats_filename = f"statistics_{method_id}_{base_model_name}_{timestamp}.json"
        stats_file_path = os.path.join("outputs", stats_filename)
        
        # ç¡®ä¿outputsç›®å½•å­˜åœ¨
        os.makedirs("outputs", exist_ok=True)
        
        with open(stats_file_path, "w", encoding="utf-8") as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        
        return stats_file_path
    
    def _extract_token_statistics(self, generation_result: Dict[str, Any], 
                                second_generation_result: Dict[str, Any]) -> Dict[str, Any]:
        """ä»ç”Ÿæˆç»“æœä¸­æå–tokenç»Ÿè®¡ä¿¡æ¯"""
        token_stats = {
            "first_generation": {
                "total_prompt_tokens": 0,
                "total_completion_tokens": 0,
                "total_tokens": 0,
                "total_cost": 0.0
            },
            "second_generation": {
                "total_prompt_tokens": 0,
                "total_completion_tokens": 0,
                "total_tokens": 0,
                "total_cost": 0.0
            },
            "combined": {
                "total_prompt_tokens": 0,
                "total_completion_tokens": 0,
                "total_tokens": 0,
                "total_cost": 0.0
            }
        }
        
        # ä»ç¬¬ä¸€æ¬¡ç”Ÿæˆç»“æœæå–tokenä¿¡æ¯
        if generation_result.get("success") and generation_result.get("output_file"):
            try:
                with open(generation_result["output_file"], "r", encoding="utf-8") as f:
                    data = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯V3æ ¼å¼ï¼ˆåŒ…å«cost_infoå­—æ®µçš„æ•°ç»„ï¼‰
                    if isinstance(data, list) and len(data) > 0:
                        # V3æ ¼å¼ï¼šä»æ¯ä¸ªé¢˜ç›®çš„cost_infoå­—æ®µç´¯è®¡
                        total_prompt = 0
                        total_completion = 0
                        total_tokens = 0
                        total_cost = 0.0
                        
                        for item in data:
                            if isinstance(item, dict) and "cost_info" in item:
                                cost_info = item["cost_info"]
                                total_prompt += cost_info.get("prompt_tokens", 0)
                                total_completion += cost_info.get("completion_tokens", 0)
                                total_tokens += cost_info.get("total_tokens", 0)
                                total_cost += cost_info.get("total_cost", 0.0)
                        
                        token_stats["first_generation"] = {
                            "total_prompt_tokens": total_prompt,
                            "total_completion_tokens": total_completion,
                            "total_tokens": total_tokens,
                            "total_cost": total_cost
                        }
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯baselineæ ¼å¼ï¼ˆåŒ…å«token_usageå­—æ®µçš„å­—å…¸ï¼‰
                    elif isinstance(data, dict) and "token_usage" in data:
                        usage = data["token_usage"]
                        token_stats["first_generation"] = {
                            "total_prompt_tokens": usage.get("total_prompt_tokens", 0),
                            "total_completion_tokens": usage.get("total_completion_tokens", 0),
                            "total_tokens": usage.get("total_tokens", 0),
                            "total_cost": usage.get("total_cost", 0.0)
                        }
                        
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•ä»ç¬¬ä¸€æ¬¡ç”Ÿæˆæ–‡ä»¶æå–tokenç»Ÿè®¡: {e}")
        
        # ä»ç¬¬äºŒæ¬¡ç”Ÿæˆç»“æœæå–tokenä¿¡æ¯
        if second_generation_result and second_generation_result.get("success") and second_generation_result.get("output_file"):
            try:
                with open(second_generation_result["output_file"], "r", encoding="utf-8") as f:
                    data = json.load(f)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯V3æ ¼å¼ï¼ˆåŒ…å«cost_infoå­—æ®µçš„æ•°ç»„ï¼‰
                    if isinstance(data, list) and len(data) > 0:
                        # V3æ ¼å¼ï¼šä»æ¯ä¸ªé¢˜ç›®çš„cost_infoå­—æ®µç´¯è®¡
                        total_prompt = 0
                        total_completion = 0
                        total_tokens = 0
                        total_cost = 0.0
                        
                        for item in data:
                            if isinstance(item, dict) and "cost_info" in item:
                                cost_info = item["cost_info"]
                                total_prompt += cost_info.get("prompt_tokens", 0)
                                total_completion += cost_info.get("completion_tokens", 0)
                                total_tokens += cost_info.get("total_tokens", 0)
                                total_cost += cost_info.get("total_cost", 0.0)
                        
                        token_stats["second_generation"] = {
                            "total_prompt_tokens": total_prompt,
                            "total_completion_tokens": total_completion,
                            "total_tokens": total_tokens,
                            "total_cost": total_cost
                        }
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯baselineæ ¼å¼ï¼ˆåŒ…å«token_usageå­—æ®µçš„å­—å…¸ï¼‰
                    elif isinstance(data, dict) and "token_usage" in data:
                        usage = data["token_usage"]
                        token_stats["second_generation"] = {
                            "total_prompt_tokens": usage.get("total_prompt_tokens", 0),
                            "total_completion_tokens": usage.get("total_completion_tokens", 0),
                            "total_tokens": usage.get("total_tokens", 0),
                            "total_cost": usage.get("total_cost", 0.0)
                        }
                        
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•ä»ç¬¬äºŒæ¬¡ç”Ÿæˆæ–‡ä»¶æå–tokenç»Ÿè®¡: {e}")
        
        # è®¡ç®—åˆè®¡
        token_stats["combined"] = {
            "total_prompt_tokens": token_stats["first_generation"]["total_prompt_tokens"] + token_stats["second_generation"]["total_prompt_tokens"],
            "total_completion_tokens": token_stats["first_generation"]["total_completion_tokens"] + token_stats["second_generation"]["total_completion_tokens"],
            "total_tokens": token_stats["first_generation"]["total_tokens"] + token_stats["second_generation"]["total_tokens"],
            "total_cost": token_stats["first_generation"]["total_cost"] + token_stats["second_generation"]["total_cost"]
        }
        
        return token_stats
    

def create_cli_interface():
    """åˆ›å»ºå‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€ç”Ÿæˆå’Œæµ‹è¯„ç³»ç»Ÿ")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--method", "-m", required=True, 
                       help="ç”Ÿæˆæ–¹æ³•ID (ä½¿ç”¨ --list-methods æŸ¥çœ‹æ‰€æœ‰æ–¹æ³•)")
    parser.add_argument("--data-file", "-d", 
                       help="æ•°æ®æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å¯¹åº”é¢˜å‹çš„æ ‡å‡†æ•°æ®)")
    parser.add_argument("--start-index", "-s", type=int, default=0,
                       help="å¼€å§‹ç´¢å¼• (é»˜è®¤: 0)")
    parser.add_argument("--end-index", "-e", type=int,
                       help="ç»“æŸç´¢å¼• (å¯é€‰)")
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--batch-size", "-b", type=int, default=1,
                       help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 1, é€‚é…Qwen-72B rate limit)")
    parser.add_argument("--delay", type=float, default=2.0,
                       help="æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•° (é»˜è®¤: 2.0, RPM:1000, TPM:20000)")
    parser.add_argument("--no-rag", action="store_true",
                       help="ç¦ç”¨RAG")
    parser.add_argument("--rag-mode", choices=["planner", "writer", "writer_only", "planner_kg"],
                       default="planner", help="RAGæ¨¡å¼ (é»˜è®¤: planner)")
    parser.add_argument("--evaluation-mode", choices=["score", "binary"],
                       default="binary", help="è¯„ä¼°æ¨¡å¼ (é»˜è®¤: binary)")
    
    # æµ‹è¯„å‚æ•°
    parser.add_argument("--no-consistency", action="store_true",
                       help="ç¦ç”¨ä¸€è‡´æ€§æµ‹è¯„")
    parser.add_argument("--no-winrate", action="store_true",
                       help="ç¦ç”¨Win Rateæµ‹è¯„")
    parser.add_argument("--enable-diversity", action="store_true",
                       help="å¯ç”¨å¤šæ ·æ€§æµ‹è¯„")
    parser.add_argument("--evaluation-model", nargs="+", default=["gpt-4o"],
                       help="æµ‹è¯„ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹ (é»˜è®¤: gpt-4o)")
    parser.add_argument("--base-model-name", choices=["gemini", "qwen-72b", "gpt-4o-mini"], default="gemini",
                       help="åŸºç¡€æ¨¡å‹åç§° (é»˜è®¤: gemini)")
    
    # æ“ä½œæ¨¡å¼
    parser.add_argument("--generate-only", action="store_true",
                       help="ä»…ç”Ÿæˆé¢˜ç›®ï¼Œä¸è¿›è¡Œæµ‹è¯„")
    parser.add_argument("--evaluate-only", action="store_true",
                       help="ä»…æµ‹è¯„å·²æœ‰æ–‡ä»¶")
    parser.add_argument("--generated-file", 
                       help="è¦æµ‹è¯„çš„ç”Ÿæˆæ–‡ä»¶è·¯å¾„ (ç”¨äº --evaluate-only)")
    
    # ä¿¡æ¯æŸ¥çœ‹
    parser.add_argument("--list-methods", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ç”Ÿæˆæ–¹æ³•")
    parser.add_argument("--list-data", action="store_true",
                       help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ•°æ®æ–‡ä»¶")
    
    return parser

async def main():
    """ä¸»å‡½æ•°"""
    parser = create_cli_interface()
    args = parser.parse_args()
    
    # åˆ›å»ºç»Ÿä¸€ç³»ç»Ÿ
    system = UnifiedGenerationEvaluation()
    
    # å¤„ç†ä¿¡æ¯æŸ¥çœ‹å‘½ä»¤
    if args.list_methods:
        system.list_methods()
        return
    
    if args.list_data:
        system.list_data_files()
        return
    
    # éªŒè¯å¿…è¦å‚æ•°
    if not args.method:
        print("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®šç”Ÿæˆæ–¹æ³• (--method)")
        print("ä½¿ç”¨ --list-methods æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ–¹æ³•")
        return
    
    # ä»…æµ‹è¯„æ¨¡å¼
    if args.evaluate_only:
        if not args.generated_file:
            print("âŒ é”™è¯¯: --evaluate-only éœ€è¦æŒ‡å®š --generated-file")
            return
        
        result = system.evaluate_questions(
            method_id=args.method,
            generated_file=args.generated_file,
            start_index=args.start_index,
            end_index=args.end_index,
            enable_consistency=not args.no_consistency,
            enable_winrate=not args.no_winrate,
            enable_diversity=args.enable_diversity,
            evaluation_model=args.evaluation_model
        )
        
        if result["success"]:
            print("âœ… æµ‹è¯„å®Œæˆ!")
            print(f"ğŸ“Š æµ‹è¯„æ–‡ä»¶: {result['evaluation_file']}")
        else:
            print(f"âŒ æµ‹è¯„å¤±è´¥: {result['error']}")
        return
    
    # ç”Ÿæˆæ¨¡å¼
    if args.generate_only:
        result = await system.generate_questions(
            method_id=args.method,
            data_file=args.data_file,
            start_index=args.start_index,
            end_index=args.end_index,
            batch_size=args.batch_size,
            delay_between_batches=args.delay,
            use_rag=not args.no_rag,
            rag_mode=args.rag_mode,
            evaluation_mode=args.evaluation_mode,
            base_model_name=args.base_model_name
        )
        
        if result["success"]:
            print("âœ… ç”Ÿæˆå®Œæˆ!")
            print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {result['output_file']}")
            if result.get("workflow_file"):
                print(f"ğŸ“‹ Workflowæ–‡ä»¶: {result['workflow_file']}")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
        return
    
    # å®Œæ•´æµç¨‹
    result = await system.run_complete_pipeline(
        method_id=args.method,
        data_file=args.data_file,
        start_index=args.start_index,
        end_index=args.end_index,
        batch_size=args.batch_size,
        delay_between_batches=args.delay,
        use_rag=not args.no_rag,
        rag_mode=args.rag_mode,
        evaluation_mode=args.evaluation_mode,
        enable_consistency=not args.no_consistency,
        enable_winrate=not args.no_winrate,
        enable_diversity=args.enable_diversity,
        evaluation_model=args.evaluation_model,
        base_model_name=args.base_model_name
    )
    
    if result["success"]:
        print("âœ… å®Œæ•´æµç¨‹å®Œæˆ!")
        print(f"ğŸ¤– ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹: {args.base_model_name}")
        print(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶: {result['generation']['output_file']}")
        if result["generation"].get("workflow_file"):
            print(f"ğŸ“‹ Workflowæ–‡ä»¶: {result['generation']['workflow_file']}")
        
        # æ˜¾ç¤ºå¤šæ¨¡å‹æµ‹è¯„ç»“æœ
        if "evaluation_results" in result:
            print("ğŸ“Š æµ‹è¯„ç»“æœ:")
            for model, eval_result in result["evaluation_results"].items():
                if eval_result["success"]:
                    print(f"  âœ… {model}: {eval_result['evaluation_file']}")
                else:
                    print(f"  âŒ {model}: {eval_result.get('error', 'æµ‹è¯„å¤±è´¥')}")
        elif "evaluation" in result and result["evaluation"].get("evaluation_file"):
            print(f"ğŸ“Š æµ‹è¯„æ–‡ä»¶: {result['evaluation']['evaluation_file']}")
        
        # æ˜¾ç¤ºç»Ÿè®¡æ–‡ä»¶
        if "statistics_file" in result:
            print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {result['statistics_file']}")
    else:
        print(f"âŒ æµç¨‹å¤±è´¥: {result['error']}")
        if "evaluation_results" in result:
            print("ğŸ“Š æµ‹è¯„å¤±è´¥è¯¦æƒ…:")
            for model, eval_result in result["evaluation_results"].items():
                print(f"  âŒ {model}: {eval_result.get('error', 'æµ‹è¯„å¤±è´¥')}")

if __name__ == "__main__":
    print("ğŸ” è°ƒè¯•ä¿¡æ¯: å¼€å§‹æ‰§è¡Œä¸»ç¨‹åº")
    print("ğŸ” è°ƒè¯•ä¿¡æ¯: è¿™æ˜¯ unified_generation_evaluation.py æ–‡ä»¶")
    
    try:
        # ===== æ‰‹åŠ¨è®¾ç½®å‚æ•°åŒºåŸŸ =====
        # å¦‚æœä½ æƒ³ç›´æ¥åœ¨ç¼–è¾‘å™¨ä¸­è¿è¡Œï¼Œå¯ä»¥ä¿®æ”¹ä¸‹é¢çš„å‚æ•°
        MANUAL_RUN = True  # è®¾ç½®ä¸º True å¯ç”¨æ‰‹åŠ¨å‚æ•°æ¨¡å¼
        
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯: MANUAL_RUN = {MANUAL_RUN}")
        
        if MANUAL_RUN:
            print("ğŸ” è°ƒè¯•ä¿¡æ¯: è¿›å…¥æ‰‹åŠ¨å‚æ•°æ¨¡å¼")
            # ===== æ‰‹åŠ¨è®¾ç½®å‚æ•°åŒºåŸŸ =====
            # ä½ å¯ä»¥ä¿®æ”¹ä¸‹é¢çš„å‚æ•°æ¥å®šåˆ¶è¿è¡Œè¡Œä¸º
            manual_params = {
                # === åŸºæœ¬å‚æ•° ===
                "method_id": "react_choice",  # ç”Ÿæˆæ–¹æ³•ID
                # å¯é€‰å€¼: v3_choice, v3_blank, cot_choice, cot_blank, cot_n_choice, cot_n_blank, react_choice, react_blank
                
                "start_index": 0,          # å¼€å§‹ç´¢å¼• (ä»ç¬¬å‡ é“é¢˜å¼€å§‹)
                "end_index": 2,           # ç»“æŸç´¢å¼• (åˆ°ç¬¬å‡ é“é¢˜ç»“æŸï¼ŒNoneè¡¨ç¤ºå¤„ç†åˆ°æœ€å)
                
                # === ç”Ÿæˆå‚æ•° ===
                "batch_size": 5,           # æ‰¹å¤„ç†å¤§å° (æ¯æ¬¡å¤„ç†å¤šå°‘é“é¢˜) - é€‚é…Qwen-72B rate limit
                "delay_between_batches": 1.0,  # æ‰¹æ¬¡é—´å»¶è¿Ÿç§’æ•° (é¿å…APIé™åˆ¶) - RPM:1000, TPM:20000
                
                # === RAGå‚æ•° ===
                "use_rag": True,           # æ˜¯å¦ä½¿ç”¨RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)
                "rag_mode": "writer",     # RAGæ¨¡å¼: planner, writer, writer_only, planner_kg
                "evaluation_mode": "binary",  # è¯„ä¼°æ¨¡å¼: score, binary
                
                # === æµ‹è¯„å‚æ•° ===
                "enable_consistency": True,    # æ˜¯å¦å¯ç”¨ä¸€è‡´æ€§æµ‹è¯„
                "enable_winrate": True,       # æ˜¯å¦å¯ç”¨Win Rateæµ‹è¯„
                "enable_diversity": True,     # æ˜¯å¦å¯ç”¨å¤šæ ·æ€§æµ‹è¯„
                "run_twice_for_consistency": True,  # æ˜¯å¦è¿›è¡Œä¸¤æ¬¡ç”Ÿæˆä»¥æµ‹è¯„å¤šæ ·æ€§
                "evaluation_models": ["gpt-4o", "deepseek-v3"],  # æµ‹è¯„ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ”¯æŒå¤šä¸ªæ¨¡å‹
                "base_model_name": "gpt-4o-mini",  # åŸºç¡€æ¨¡å‹åç§°: gemini, qwen-72b æˆ– gpt-4o-mini
                
                # === æ•°æ®æ–‡ä»¶ ===
                "data_file": None  # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤æ•°æ®æ–‡ä»¶
                # é€‰æ‹©é¢˜é»˜è®¤: D:\CODE\three_0921\data\choice_unquie_500.jsonl
                # å¡«ç©ºé¢˜é»˜è®¤: D:\CODE\three_0921\data\blank_unique.jsonl
            }
            
            async def manual_run():
                """æ‰‹åŠ¨è¿è¡Œæ¨¡å¼"""
                print("ğŸš€ ä½¿ç”¨æ‰‹åŠ¨å‚æ•°æ¨¡å¼è¿è¡Œ...")
                print(f"ğŸ“‹ æ–¹æ³•: {manual_params['method_id']}")
                print(f"ğŸ“Š èŒƒå›´: {manual_params['start_index']}-{manual_params['end_index']}")
                print(f"ğŸ”§ æ‰¹å¤„ç†å¤§å°: {manual_params['batch_size']}")
                print(f"ğŸ”§ RAGæ¨¡å¼: {manual_params['rag_mode']}")
                print("=" * 50)
                
                system = UnifiedGenerationEvaluation()
                result = await system.run_with_params(**manual_params)
                
                if result["success"]:
                    print("âœ… å®Œæ•´æµç¨‹å®Œæˆ!")
                    print(f"ğŸ¤– ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹: {manual_params['base_model_name']}")
                    print(f"ğŸ“ ç¬¬ä¸€æ¬¡ç”Ÿæˆæ–‡ä»¶: {result['generation']['output_file']}")
                    if result.get("second_generation"):
                        print(f"ğŸ“ ç¬¬äºŒæ¬¡ç”Ÿæˆæ–‡ä»¶: {result['second_generation']['output_file']}")
                    if result["generation"].get("workflow_file"):
                        print(f"ğŸ“‹ Workflowæ–‡ä»¶: {result['generation']['workflow_file']}")
                    
                    # æ˜¾ç¤ºå¤šæ¨¡å‹æµ‹è¯„ç»“æœ
                    if "evaluation_results" in result:
                        print("ğŸ“Š æµ‹è¯„ç»“æœ:")
                        for model, eval_result in result["evaluation_results"].items():
                            if eval_result["success"]:
                                print(f"  âœ… {model}: {eval_result['evaluation_file']}")
                            else:
                                print(f"  âŒ {model}: {eval_result.get('error', 'æµ‹è¯„å¤±è´¥')}")
                    elif "evaluation" in result and result["evaluation"].get("evaluation_file"):
                        print(f"ğŸ“Š æµ‹è¯„æ–‡ä»¶: {result['evaluation']['evaluation_file']}")
                    
                    # æ˜¾ç¤ºç»Ÿè®¡æ–‡ä»¶
                    if "statistics_file" in result:
                        print(f"ğŸ“Š ç»Ÿè®¡æ–‡ä»¶: {result['statistics_file']}")
                else:
                    print(f"âŒ æµç¨‹å¤±è´¥: {result['error']}")
                    if "evaluation_results" in result:
                        print("ğŸ“Š æµ‹è¯„å¤±è´¥è¯¦æƒ…:")
                        for model, eval_result in result["evaluation_results"].items():
                            print(f"  âŒ {model}: {eval_result.get('error', 'æµ‹è¯„å¤±è´¥')}")
                
                return result
            
            asyncio.run(manual_run())
        else:
            # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼
            asyncio.run(main())
    except Exception as e:
        print(f"âŒ è°ƒè¯•ä¿¡æ¯: ä¸»ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
