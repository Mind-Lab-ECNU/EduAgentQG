#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„çŸ¥è¯†æ£€ç´¢å™¨
"""

import re
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

logger = logging.getLogger(__name__)

class SemanticRetriever:
    """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„çŸ¥è¯†æ£€ç´¢å™¨"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–è¯­ä¹‰æ£€ç´¢å™¨
        
        Args:
            model_name: å¥å­åµŒå…¥æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.model = None
        self.math_knowledge_data = ""
        self.knowledge_chunks = []
        self.chunk_embeddings = None
        
        # åˆå§‹åŒ–æ•°æ®
        self._load_data()
        self._initialize_model()
        self._prepare_chunks()
    
    def _load_data(self):
        """åŠ è½½MDæ–‡ä»¶æ•°æ®"""
        try:
            current_dir = Path(__file__).parent
            data_dir = current_dir / "data"
            math_knowledge_file = data_dir / "è¯¾æ ‡miner.md"
            
            if math_knowledge_file.exists():
                with open(math_knowledge_file, 'r', encoding='utf-8') as f:
                    self.math_knowledge_data = f.read()
                logger.info(f"æˆåŠŸåŠ è½½MDæ–‡ä»¶: {math_knowledge_file}")
            else:
                logger.warning(f"MDæ–‡ä»¶ä¸å­˜åœ¨: {math_knowledge_file}")
                
        except Exception as e:
            logger.error(f"åŠ è½½MDæ–‡ä»¶å¤±è´¥: {e}")
    
    def _initialize_model(self):
        """åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹"""
        try:
            self.model = SentenceTransformer(self.model_name)
            logger.info(f"æˆåŠŸåˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹: {self.model_name}")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹å¤±è´¥: {e}")
            # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬åŒ¹é…ä½œä¸ºå¤‡é€‰
            self.model = None
    
    def _prepare_chunks(self):
        """å°†MDå†…å®¹åˆ†å‰²æˆè¯­ä¹‰å—å¹¶ç”ŸæˆåµŒå…¥"""
        if not self.math_knowledge_data or not self.model:
            return
        
        try:
            # æŒ‰æ®µè½åˆ†å‰²å†…å®¹
            paragraphs = self._split_into_paragraphs(self.math_knowledge_data)
            
            # è¿‡æ»¤å’Œæ¸…ç†æ®µè½
            self.knowledge_chunks = []
            for para in paragraphs:
                if self._is_relevant_paragraph(para):
                    cleaned_para = self._clean_paragraph(para)
                    if cleaned_para and len(cleaned_para) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        self.knowledge_chunks.append(cleaned_para)
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            if self.knowledge_chunks:
                self.chunk_embeddings = self.model.encode(self.knowledge_chunks)
                logger.info(f"æˆåŠŸç”Ÿæˆ {len(self.knowledge_chunks)} ä¸ªçŸ¥è¯†å—çš„åµŒå…¥å‘é‡")
            else:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„çŸ¥è¯†å—")
                
        except Exception as e:
            logger.error(f"å‡†å¤‡çŸ¥è¯†å—å¤±è´¥: {e}")
    
    def _split_into_paragraphs(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½"""
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        
        # è¿›ä¸€æ­¥æŒ‰æ ‡é¢˜åˆ†å‰²
        result = []
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœæ®µè½å¾ˆé•¿ï¼ŒæŒ‰å¥å­è¿›ä¸€æ­¥åˆ†å‰²
            if len(para) > 500:
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', para)
                current_chunk = ""
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    if len(current_chunk + sentence) > 300:
                        if current_chunk:
                            result.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        current_chunk += sentence + "ã€‚"
                if current_chunk:
                    result.append(current_chunk.strip())
            else:
                result.append(para)
        
        return result
    
    def _is_relevant_paragraph(self, para: str) -> bool:
        """åˆ¤æ–­æ®µè½æ˜¯å¦ä¸æ•°å­¦æ•™è‚²ç›¸å…³"""
        # æ’é™¤æ˜æ˜¾ä¸ç›¸å…³çš„å†…å®¹
        exclude_keywords = [
            "é½é²æ ¡å›­", "å…³æ³¨", "å¾®ä¿¡å…¬ä¼—å·", "äºŒç»´ç ", "å›¾ç‰‡", "è¡¨æ ¼",
            "ç»­è¡¨", "å›¾", "è¡¨", "![](images/", "http", "www."
        ]
        
        for keyword in exclude_keywords:
            if keyword in para:
                return False
        
        # åŒ…å«æ•°å­¦æ•™è‚²ç›¸å…³å…³é”®è¯
        math_keywords = [
            "æ•°å­¦", "è¿ç®—", "å‡ ä½•", "æ¨ç†", "æ•°æ®", "ç´ å…»", "èƒ½åŠ›", "æ„è¯†",
            "å­¦ä¹ ", "æ•™å­¦", "è¯¾ç¨‹", "æ ‡å‡†", "ç›®æ ‡", "è¦æ±‚", "å»ºè®®", "è¯„ä»·",
            "å°å­¦", "åˆä¸­", "å¹´çº§", "å­¦ç”Ÿ", "æ•™å¸ˆ", "æ•™è‚²"
        ]
        
        return any(keyword in para for keyword in math_keywords)
    
    def _clean_paragraph(self, para: str) -> str:
        """æ¸…ç†æ®µè½å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        para = re.sub(r'\s+', ' ', para)
        
        # ç§»é™¤è¡¨æ ¼æ ‡è®°
        para = re.sub(r'<table>.*?</table>', '', para, flags=re.DOTALL)
        para = re.sub(r'<tr>.*?</tr>', '', para, flags=re.DOTALL)
        para = re.sub(r'<td>.*?</td>', '', para, flags=re.DOTALL)
        
        # ç§»é™¤å›¾ç‰‡æ ‡è®°
        para = re.sub(r'!\[.*?\]\(.*?\)', '', para)
        
        return para.strip()
    
    def search_semantic(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢ç›¸å…³å†…å®¹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸å…³çš„ç»“æœ
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸å…³å†…å®¹çš„åˆ—è¡¨
        """
        if not self.model or not self.chunk_embeddings is not None:
            logger.warning("è¯­ä¹‰æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…")
            return self._fallback_keyword_search(query)
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode([query])
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.chunk_embeddings)[0]
            
            # è·å–æœ€ç›¸å…³çš„ç»“æœ
            results = []
            for i, similarity in enumerate(similarities):
                if similarity >= threshold:
                    results.append({
                        'content': self.knowledge_chunks[i],
                        'similarity': float(similarity),
                        'index': i
                    })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰kä¸ª
            results.sort(key=lambda x: x['similarity'], reverse=True)
            return results[:top_k]
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return self._fallback_keyword_search(query)
    
    def _fallback_keyword_search(self, query: str) -> List[Dict[str, Any]]:
        """å¤‡é€‰çš„å…³é”®è¯æœç´¢æ–¹æ³•"""
        results = []
        query_lower = query.lower()
        
        for i, chunk in enumerate(self.knowledge_chunks):
            chunk_lower = chunk.lower()
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            if any(word in chunk_lower for word in query_lower.split()):
                results.append({
                    'content': chunk,
                    'similarity': 0.5,  # ç»™ä¸€ä¸ªé»˜è®¤ç›¸ä¼¼åº¦
                    'index': i
                })
        
        return results[:5]
    
    def search_curriculum_standards_semantic(self, knowledge_point: str, difficulty: str, 
                                           core_competency: str, grade: str, grade_level: str) -> Dict[str, Any]:
        """
        åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢è¯¾ç¨‹æ ‡å‡†
        
        Args:
            knowledge_point: çŸ¥è¯†ç‚¹
            difficulty: éš¾åº¦
            core_competency: æ ¸å¿ƒç´ å…»
            grade: å¹´çº§
            grade_level: å¹´çº§çº§åˆ«
            
        Returns:
            è¯¾ç¨‹æ ‡å‡†æ£€ç´¢ç»“æœ
        """
        try:
            results = {
                "knowledge_point": knowledge_point,
                "difficulty": difficulty,
                "core_competency": core_competency,
                "grade": grade,
                "grade_level": grade_level,
                "curriculum_requirements": [],
                "teaching_suggestions": [],
                "learning_objectives": [],
                "assessment_criteria": []
            }
            
            # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
            queries = [
                f"{knowledge_point} {core_competency} æ•™å­¦è¦æ±‚",
                f"{knowledge_point} {core_competency} å­¦ä¹ ç›®æ ‡",
                f"{grade} {knowledge_point} è¯¾ç¨‹æ ‡å‡†",
                f"{core_competency} åŸ¹å…» æ•™å­¦å»ºè®®",
                f"{difficulty} {knowledge_point} è¯„ä»·æ ‡å‡†"
            ]
            
            # æœç´¢ç›¸å…³å†…å®¹
            all_results = []
            for query in queries:
                semantic_results = self.search_semantic(query, top_k=3, threshold=0.2)
                all_results.extend(semantic_results)
            
            # å»é‡å¹¶æŒ‰ç›¸ä¼¼åº¦æ’åº
            seen_contents = set()
            unique_results = []
            for result in all_results:
                if result['content'] not in seen_contents:
                    seen_contents.add(result['content'])
                    unique_results.append(result)
            
            unique_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # åˆ†ç±»å†…å®¹
            for result in unique_results[:10]:  # å–å‰10ä¸ªæœ€ç›¸å…³çš„ç»“æœ
                content = result['content']
                similarity = result['similarity']
                
                # æ ¹æ®å†…å®¹ç‰¹å¾åˆ†ç±»
                if any(word in content for word in ["è¦æ±‚", "æ ‡å‡†", "ç›®æ ‡", "æŒæ¡", "ç†è§£"]):
                    results["curriculum_requirements"].append(f"{content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                elif any(word in content for word in ["å»ºè®®", "æ–¹æ³•", "ç­–ç•¥", "åŸ¹å…»", "å‘å±•"]):
                    results["teaching_suggestions"].append(f"{content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                elif any(word in content for word in ["å­¦ä¹ ", "æŒæ¡", "ç†è§£", "èƒ½åŠ›", "ç´ å…»"]):
                    results["learning_objectives"].append(f"{content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                elif any(word in content for word in ["è¯„ä»·", "è¯„ä¼°", "è€ƒæ ¸", "æµ‹è¯•", "æ£€æµ‹"]):
                    results["assessment_criteria"].append(f"{content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
                else:
                    # é»˜è®¤å½’ç±»ä¸ºè¦æ±‚
                    results["curriculum_requirements"].append(f"{content} (ç›¸ä¼¼åº¦: {similarity:.3f})")
            
            # é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æ•°é‡
            for key in results:
                if isinstance(results[key], list):
                    results[key] = results[key][:3]
            
            logger.info(f"è¯­ä¹‰æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {sum(len(v) for v in results.values() if isinstance(v, list))} æ¡ç›¸å…³å†…å®¹")
            return results
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰è¯¾ç¨‹æ ‡å‡†æ£€ç´¢å¤±è´¥: {e}")
            return {
                "knowledge_point": knowledge_point,
                "error": str(e)
            }

def test_semantic_retrieval():
    """æµ‹è¯•è¯­ä¹‰æ£€ç´¢åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•è¯­ä¹‰æ£€ç´¢åŠŸèƒ½...")
    
    try:
        retriever = SemanticRetriever()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "è¿ç®—èƒ½åŠ› åŸ¹å…» æ•™å­¦å»ºè®®",
            "å‡ ä½•ç›´è§‚ ç©ºé—´è§‚å¿µ å­¦ä¹ ç›®æ ‡",
            "æ¨ç†èƒ½åŠ› é€»è¾‘æ€ç»´ è¯„ä»·æ ‡å‡†",
            "æ•°æ®è§‚å¿µ ç»Ÿè®¡ è¯¾ç¨‹è¦æ±‚"
        ]
        
        for query in test_queries:
            print(f"\nğŸ“ æŸ¥è¯¢: {query}")
            results = retriever.search_semantic(query, top_k=3)
            
            print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
            for i, result in enumerate(results, 1):
                print(f"     {i}. ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                print(f"        å†…å®¹: {result['content'][:100]}...")
        
        # æµ‹è¯•å®Œæ•´çš„è¯¾ç¨‹æ ‡å‡†æ£€ç´¢
        print(f"\nğŸ“š æµ‹è¯•è¯¾ç¨‹æ ‡å‡†è¯­ä¹‰æ£€ç´¢:")
        curriculum_results = retriever.search_curriculum_standards_semantic(
            knowledge_point="ä¹˜æ³•åˆ†é…å¾‹",
            difficulty="æ˜“",
            core_competency="è¿ç®—èƒ½åŠ›",
            grade="å››å¹´çº§ä¸‹",
            grade_level="ä¸‹"
        )
        
        for key, values in curriculum_results.items():
            if isinstance(values, list) and values:
                print(f"   {key}: {len(values)} æ¡")
                for value in values[:2]:
                    print(f"     - {value[:80]}...")
        
        print(f"\nâœ… è¯­ä¹‰æ£€ç´¢æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_semantic_retrieval()
